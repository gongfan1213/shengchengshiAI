
### 5.3 使用Transformer处理多模态大语言模型
#### 1. 构建对话上下文
这一步调用`make_context`函数，根据输入的查询、历史记录和系统信息构建对话上下文。该上下文包括初始文本和上下文token列表，便于计算机的理解和识别。示例代码如下。
### Python
```python
raw_text, context_tokens = make_context(
    tokenizer,
    query,
    history=history,
    system=system,
    max_window_size=6144,
    chat_format=self.generation_config.chat_format
)
```
在这段代码中，可以看到一些由用户调整的参数，如`max_window_size`和`chat_format`。这些参数都可以影响最后结果的生成，如`max_window_size`规定了最大的对话窗口长度，在需要输出一些比较复杂的回答时，就可以用这个参数来限定最大回答长度。
#### 2. 获取停用词的标识
这一步调用`get_stop_words_ids`函数，根据聊天格式和分词器获取停用词的标识，虽然是比较简单的代码，但是对生成对话的质量有很大的提高。因为它可以过滤掉一些无意义的词语，从而提高模型对关键词语识别的准确程度。示例代码如下。
### Python
```python
stop_words_ids = get_stop_words_ids(self.generation_config.chat_format, tokenizer)
```
#### 3. 将上下文token列表转换为PyTorch张量并生成响应
这一步将上下文token列表转换为PyTorch张量，并将其发送到适当的设备上（通常是GPU），并调用模型的`generate`方法，基于输入的上下文token列表生成响应。示例代码如下。
### Python
```python
input_ids = torch.tensor([context_tokens]).to(self.device)
outputs = self.generate(
    input_ids,
    stop_words_ids=stop_words_ids,
    return_dict_in_generate=False,
)
```
#### 4. 解码生成的响应
这一步调用`decode_tokens`函数，根据生成的token序列解码出文本形式的响应。示例代码如下。
### Python
```python
response = decode_tokens(
    outputs[0],
    tokenizer,
    raw_text_len=len(raw_text),
    context_length=len(context_tokens),
    chat_format=self.generation_config.chat_format,
    verbose=False,
)
```
`verbose`参数通常用于控制函数或方法的输出信息的详细程度。当`verbose`设置为`True`时，函数或方法可能会输出更多的信息，以帮助用户了解其内部的工作流程或者提供额外的调试信息。而当`verbose`设置为`False`时，函数或方法通常会以更简洁的方式执行，不输出过多冗余的信息。一般在调试过程中会采取`True`参数，而使用时则会默认为`False`。
#### 5. 更新历史记录并返回响应和更新后的历史记录
这一步根据参数`append_history`的值，决定是否将查询和响应添加到历史记录中，最后，返回一个字典，包含生成的响应和可能更新的历史记录。示例代码如下。
### Python
```python
if append_history:
    history.append((query, response))
return {OutputKeys.RESPONSE: response, OutputKeys.HISTORY: history}
```
### 5. 量化
本节提供了一个基于AutoGPTQ的新解决方案，并发布了Qwen-VL-Chat的Int4量化模型——Qwen-VL-Chat-Int4。这一新模型在保持近乎无损的模型效果的同时，实现了对内存成本和推理速度的双重优化，显著提升了整体性能。接下来介绍量化的具体步骤。
- **第1步**：安装运行所需的包。示例代码如下。
### Shell
```bash
pip install optimum
git clone https://github.com/JustinLin610/AutoGPTQ.git & cd AutoGPTQ
pip install -v.
```
- **第2步**：运行示例。示例代码如下。
### Python
```python
model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen-VL-Chat-Int4",
    device_map="auto",
    trust_remote_code=True
).eval()
# Either a local path or an url between <img></img> tags
image_path = './assets/imgs/picture.jpg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>描述画面内容。', history=None)
print(response)
``` 


### 5.3.3 使用ModelScope
在之前配置的基础上，需要为模型配置专门的ModelScope库。只须在项目运行环境中用pip指令安装即可。具体安装命令如下。
### Shell
```bash
pip install ModelScope
```
示例代码如下。
### Python
```python
from modelscope import (
    snapshot_download, AutoModelForCausalLM, AutoTokenizer, GenerationConfig
)
import torch
model_id = 'qwen/Qwen-VL-Chat'
revision = 'v1.0.0'
model_dir = snapshot_download(model_id, revision=revision)
torch.manual_seed(1234)
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
if not hasattr(tokenizer,'model_dir'):
    tokenizer.model_dir = model_dir
# use bf16
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="cpu", trust_remote_code=True).eval()
# use auto
# model = AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", trust_remote_code=True).eval()
# Specify hyperparameters for generation (No need to do this if you are using transformers>4.32.0)
# model.generation_config = GenerationConfig.from_pretrained(model_dir, trust_remote_code=True)
# 1st dialogue turn
# Either a local path or an url between <img></img> tags
image_path = './assets/imgs/picture.jpg'
response, history = model.chat(tokenizer, query=f'<img>{image_path}</img>描述这幅画', history=None)
print(response)
# 图中画了两幅画，一幅是真实生活中的女孩坐在椅子上的画面，另一幅是根据第一幅画绘制的线描画
# 图中是真实人物的写照，分别为线描画和色彩画两种形式
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
    image.save('1.jpg')
else:
    print("no box")
# no box
```
为了与Transformer模型进行对比，这里的ModelScope只运行了使用fp16的预训练模型。从回答结果来看，虽然两段代码在实现方面只有使用的框架存在差别，但是最后的回答结果还是有一定差异的。不过，由于在识别真人并生成图像时，ModelScope并没有正确识别，因此它的性能比Transformer模型略差。

ModelScope和Transformer模型的区别如下。
- Transformer模型以英文为主，中文模型相对较少，适用于NLP以及部分视觉和语音任务，在很多任务上表现出色。虽然也有一些中文模型，但相对于英文模型数量较少。
- ModelScope则可能更加注重中文领域的模型评估和比较，因为大部分模型名字都是中文。但是因为训练量不如Transformer模型，所以模型表现可能会较差。
### 5.4 3D生成：Stable Diffusion和Generative Gaussian Splatting方法
 
