### 第3章 Stable Diffusion

本章将介绍图像生成领域中运用最广、效果最好的模型——Stable Diffusion。

本章介绍的案例主要基于Hugging Face框架的官方样例示例代码来实现，这是工业界最规范、最简单的Stable Diffusion实现方法，其有效性得到工业界大量的验证。


#### 3.1 Stable Diffusion简介
截至撰写本书时，Stable Diffusion是最强大的图像生成模型。Stable Diffusion是一种基于人工智能的绘画软件，它利用深度学习技术，特别是生成对抗网络来生成高品质的数字艺术作品。这种技术允许软件“学习”各种艺术风格和图像特征，从而能够创造出与真实艺术家作品相似的图像。Stable Diffusion的基本原理如图3-1所示。

![image](https://github.com/user-attachments/assets/1e77e1ef-7e4a-4183-8416-6619b3b5df92)


Stable Diffusion可以被艺术家和设计师用于快速生成创意草图、视觉灵感探索或具体艺术作品的创作。它特别适用于需要大量视觉内容而时间又紧迫的情境，比如广告设计、游戏开发和电影制作等。

Stable Diffusion（WebUI）的用户界面（见图3-2）直观易用，即使是没有编程或深度学习背景的用户也可以轻松上手。用户可以通过简单的操作调整图像的风格、色彩、纹理等参数，实时查看预览效果，并对生成结果进行微调。

![image](https://github.com/user-attachments/assets/44a64094-3ed0-4b84-8c33-2b3e6d247ce0)


与其他AI绘画工具相比，Stable Diffusion提供了更加灵活和强大的功能。用户可以更细致地控制图像生成过程，创作出更符合个人风格和需求的作品。此外，它的学习能力使其能够适应广泛的艺术风格，为用户提供无限的创意可能。

##### 3.1.1 软件对比

与市面上其他主流AI绘画软件（如Midjourney）相比，Stable Diffusion在功能方面更为强大，也更容易上手。

1. **对比其他AI绘画软件**

Stable Diffusion在功能和灵活性方面与其他主流AI绘画软件（如Midjourney）有着明显的差异。虽然这些软件都使用了深度学习技术，但在细节控制、用户界面和定制能力方面，Stable Diffusion提供了更加先进的选项。

2. **灵活性和控制**

Stable Diffusion提供高度的定制性和控制能力。用户可以微调大量参数，以精确控制图像生成的各个方面，如风格、纹理、色彩等。这使得它特别适合需要精细控制的专业艺术家和设计师。

虽然其他软件（如Midjourney）也能生成高质量的图像，但通常提供较少的定制选项和控制灵活性。它们更适合于快速生成和探索创意，而不是精细制作。 

3. **上手难度**

由于Stable Diffusion提供了更多的自定义选项，因此它的上手难度相对较高。这需要用户对不同的生成参数有一定的理解，才能充分利用其强大的功能。相比之下，其他一些AI绘画工具可能更易于初学者快速上手使用。 

4. **适用场景**

Stable Diffusion因其高度的灵活性和定制性，特别适合于专业的艺术创作和设计工作。而其他AI绘画工具可能更适合于快速创作和创意探索，特别是在时间紧迫的情况下。

选择哪款AI绘画软件取决于用户的具体需求。如果需要高度定制化的图像和细致的控制，Stable Diffusion是一个优秀的选择。如果寻求快速生成和简单操作，其他软件（如Midjourney）可能更合适。



##### 3.1.2 计算机配置要求

为了确保Stable Diffusion能够高效运行，用户的计算机需要具备一定的硬件配置。以下是主要的硬件要求。

1. **显卡**

由于Stable Diffusion的绘画算法主要依赖GPU进行计算，因此需要使用NVIDIA显卡。最低要求为4GB显存，但是为了更好的性能和生成更高分辨率的图像，建议使用6GB显存（或以上）。对于专业用途，如12GB显存的RTX4090显卡将提供最佳性能。

显存的大小直接影响到能够处理的图像分辨率和图像生成速度，显存越大，生成的图像越清晰，处理速度也越快。 

2. **硬盘空间**

由于Stable Diffusion需要存储大量的模型数据，因此建议使用至少60GB的硬盘空间。

应确保有足够的空间存储下载的模型文件和生成的图像。批量处理图像时，也会暂时占用较多的硬盘空间。 

3. **操作系统**

Stable Diffusion支持Windows 10或Windows 11操作系统。


推荐使用最新的操作系统版本，以确保软件运行的稳定性和安全性。 

4. **RAM**

虽然Stable Diffusion主要依赖GPU，但拥有足够的RAM（例如16GB或更多）也有助于提高处理速度，特别是在处理多个任务或大型图像文件时。 

5. **网络连接**

对于下载模型和更新软件，稳定且快速的网络连接是必要的。

良好的硬件配置是确保Stable Diffusion能够顺利运行的关键。根据个人需求和使用目的选择合适的硬件配置，可以极大地提高工作效率和创作体验。



##### 3.1.3 安装步骤

安装Stable Diffusion是一个相对直接的过程，但需要注意几个关键步骤以确保软件能够正确安装和运行。

1. **下载安装包**
    - **第1步，获取安装包**：访问Stable Diffusion官方网站，下载最新版本的安装包。这通常包括软件本身和所需的依赖文件。
    - **第2步，选择正确的版本**：确保下载与你的操作系统和硬件配置兼容的版本。例如，对于Windows操作系统用户，应选择适用于Windows 10或Windows 11操作系统的版本。 

2. **安装依赖**

在安装Stable Diffusion之前，可能需要先安装Python和其他相关的库。这些通常在安装包中有明确的指示。

根据提供的指南安装必要的依赖文件。这可能包括特定的驱动程序和支持库，以确保软件可以正常运行。 

3. **配置和启动**

配置和启动的具体步骤如下。
    - **第1步，复制文件**：将下载的文件解压缩，并将其复制到指定的目录中，例如webui文件夹。
    - **第2步，运行安装程序**：在运行安装程序的过程中需要设置一些基本内容，如设置安装路径和选择组件等。 
    - **第3步，启动软件**：完成安装后，启动Stable Diffusion。初次启动可能需要一些时间，因为软件可能需要下载额外的数据或模型。 
    - **第4步，熟悉界面**：启动软件后，你可以花些时间熟悉用户界面和各种功能。这可能包括测试不同的模型和设置，以验证安装是否成功。 

4. **调试和问题解决**

如果在安装过程中遇到问题，可以参考以下资源。
    - **官方文档**：查阅Stable Diffusion的官方文档，了解常见问题及其解决方案。
    - **社区支持**：加入相关的在线论坛或社群，获取来自其他用户的帮助和建议。 
    - **更新和补丁**：定期检查软件更新和补丁，以修复已知的问题和改进性能。 

安装Stable Diffusion可能需要一定的技术知识，特别是在配置和依赖管理方面。遵循安装指南和处理任何潜在的问题，将有助于确保软件的顺利运行。

##### 3.1.4 基础操作

Stable Diffusion的操作涉及多个方面，从模型选择到图像参数调整，每个步骤都对最终生成的图像有重要影响。

1. **模型选择和切换**

不同的模型对应着不同的艺术风格和图像类型。选择合适的模型是产生理想图像的关键步骤。

在Stable Diffusion的界面左上角，用户可以轻松切换到不同的预安装模型。这些模型可能包括特定风格的图像生成模型，如动漫风格、写实风格等。

用户可以从官方网站或社区分享的资源（见图3-3）下载更多模型。安装新模型通常涉及将模型文件放置在特定的目录中，并在软件中进行选择。 

![image](https://github.com/user-attachments/assets/eb48ce8c-c7f0-4019-95d5-720e6f2b601b)


2. **使用VAE**

VAE是Stable Diffusion中的一个重要组件，用于提高图像的质量和细节。它通过学习大量图像数据，提高生成图像的真实感和视觉效果。

用户可以在界面中选择不同的VAE设置，以适应不同类型的图像生成需求。 

3. **关键词设置**

在Stable Diffusion中，关键词用于指导AI生成特定风格或主题的图像（见图3-4）。

- **提示**：用户可以在提示框内输入描述所需图像的关键词，如“雪景”“未来城市”等。
- **反向提示**：用于排除不希望出现在图像中的元素或特征，提高生成图像的准确性。 

![image](https://github.com/user-attachments/assets/2fd833d3-d0fd-4637-9515-c0d9b604cb12)



接下来通过一个实例进行说明。

**提示**：

```
The personification of the Halloween holiday in the form of a cute girl with short hair, (((cute girl)))cute hats, cute cheeks, unreal engine, highly detailed, artgerm digital illustration, woo tooth, studio ghibli, deviantart, sharp focus, artstation, by Alexei Vinogradov bakery, sweets, emerald eyes.
```

运行上述代码，输出如图3-5所示。

![image](https://github.com/user-attachments/assets/5d3fdc3d-265d-473f-91ab-de06d7efab8c)


4. **采样步数和方法**

采样步数决定了生成过程中迭代的次数。步数越多，图像细节通常越丰富，但生成时间也会相应增长。

不同的采样方法影响生成图像的风格和质量。用户可以根据需要选择适合的方法。 

5. **高清修复**

对基础生成的图像进行高清修复，提高分辨率和细节质量。

特别适用于需要高分辨率输出的场景，如打印和专业展示。


通过熟悉和掌握这些基本操作，用户可以充分利用Stable Diffusion的强大功能，创作出符合个人风格和需求的独特艺术作品。

##### 3.1.5 高级功能

Stable Diffusion不仅限于基础的图像生成，它还提供了一系列高级功能，让用户可以进行更深入的探索和创作。

1. **模型合并**

模型合并允许用户将多个模型的特性结合在一起，创造出独特的图像风格。这种方法可以 


### 3.1.5 高级功能

1. **模型合并**

模型合并允许用户将多个模型的特性结合在一起，创造出独特的图像风格。这种方法可以产生新颖且富有创意的视觉效果，适用于实验性艺术创作或寻找新的视觉表达方式。

2. **自训练模型**

对于有特定需求或想探索特定风格的高级用户，Stable Diffusion提供了训练个性化模型的能力。用户可以用自己的图像数据集训练模型，创建出完全符合个人风格和喜好的图像生成模型。

3. **插件扩展**

Stable Diffusion支持多种插件，这些插件可以增强软件的功能，如提高图像质量、添加特殊效果等。用户可以从社区或官方渠道获取这些插件，根据自己的需要进行安装和配置。 

4. **工作流集成**

高级用户可以将Stable Diffusion集成到他们的工作流中，如与其他图像处理软件的结合，实现更加高效和专业的图像生成和编辑流程。

通过掌握这些高级功能，用户可以将Stable Diffusion的应用提升到一个全新的水平，从而在数字艺术创作中实现更大的创新和个性化表达。

Stable Diffusion作为一款先进的AI绘画软件，凭借其深度学习技术和用户友好的界面，在数字艺术领域展现了巨大的潜力。它不仅为艺术家和设计师提供了一个强大的工具来加速创意过程，而且还开辟了探索新艺术形式和表达方式的可能性。

虽然Stable Diffusion提供了众多直观且强大的功能，但要充分利用这些功能，用户需要投入时间来学习和实验。理解不同的模型、掌握关键词设置和熟悉高级功能，都是实现高质量图像生成的关键。

Stable Diffusion的发展离不开活跃的用户社区和不断的技术更新。用户可以期待更多的模型更新、新功能添加和性能改进。同时，社区的分享和交流也是学习和提升技能的宝贵资源。

无论是专业艺术家还是设计爱好者，Stable Diffusion都为其提供了一个探索创意边界的平台。它鼓励用户释放想象力，创造出独一无二的艺术作品。

Stable Diffusion是探索数字艺术未来的一扇窗口。通过不断学习和实验，每个人都可以利用这款强大的工具，将个人的艺术愿景转化为令人惊叹的视觉作品。


### 3.2 Stable Diffusion入门

为了便于理解Stable Diffusion原理，本节将介绍如下内容。

- 使用提示生成作品。
- 观察生成过程中各参数的影响。
- 对模型图像生成过程和潜在特征可视化。
- 深入了解采样函数的内部机制。
- 探究和理解Stable Diffusion模型内部的网络结构。

#### 3.2.1 前期准备

1. **环境配置**

这里默认你已经配置好对应版本的PyTorch和其他基本开发环境（更多信息参见第1章）。下面主要对运行Stable Diffusion所需要的开发环境进行介绍。运行以下命令即可完成环境的配置。

```bash

pip install diffusers transformers tokenizers
```

2. **预训练权重下载**

需要注意的是，如果想要使用预训练权重，需先登录Hugging Face账户。你可以在Hugging Face注册一个账户，并利用一个访问Token在接下来的代码块中进行登录。具体命令如下。

```python
from huggingface_hub import notebook_login
notebook_login()
```

3. **硬件配置确认**

为了保证处理图像的效率，需要用下方的代码确认运行环境支持GPU。
```python
import torch
assert torch.cuda.is_available()
!nvidia-smi
```


#### 3.2.2 加载Stable Diffusion

这里加载的是fp16格式的checkpoint（检查点），目的是节省内存和计算时间。如果你拥有性能强大的GPU，可以去掉“revision='fp16', torch_dtype=torch.float16”这行代码。示例代码如下。（与源码相比，这里的代码做了改动，因为升级了软件，所以相应的参数也进行了更改，如revision改为variant，False改为None。）

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import autocast
from diffusers import StableDiffusionPipeline
import matplotlib.pyplot as plt

def plt_show_image(image):
    plt.figure(figsize=(8, 8))
    plt.imshow(image)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

assert torch.cuda.is_available()
!nvidia-smi

pipe = StableDiffusionPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    use_auth_token=True,
    variant='fp16', torch_dtype=torch.float16
).to("cuda")
# Disable the safety checkers
def dummy_checker(images, **kwargs): return images, None
pipe.safety_checker = dummy_checker
```

1. **初步尝试**

首先通过一段简单的示例代码直观感受一下模型的效果。
```python
prompt = "a lovely cat running in the desert in Van Gogh style, trending art."
image = pipe(prompt).images[0]  # image here is in [PIL format]

# Now to display an image you can do either save it such as:
image.save(f"lovely_cat.png")
image
```

![image](https://github.com/user-attachments/assets/ed2ea0e5-7ea0-4937-ae99-2a1b8d1fe240)


运行上述代码，输出如图3-6所示。再次运行上述代码，输出如图3-7所示。

![image](https://github.com/user-attachments/assets/a60699a4-1a05-4abc-8d14-b8635d1ac3ab)

从图3-6和图3-7所示的结果可以看出，Stable Diffusion在生成图像时引入了随机性。即使使用相同的文本提示，由于初始化的随机噪声或模型内部的随机处理，每次生成的图像可能都存在细微的差异。这种随机性是生成模型的一个特点，它使得每次生成的图像都是独一无二的。 

2. **控制随机性**

上面提到，Stable Diffusion在生成图像时引入了随机性，而随机性就是通过在生成图像前，预设一个随机种子来实现的。将随机种子进行固定，多次运行代码，可以发现生成的图像与图3-8一样。由此可知，可以向pipe函数传入generator参数，从而消除生成图像的随机性。 

![image](https://github.com/user-attachments/assets/70e585ce-ec94-4161-9cb4-c3f485c0d401)


3. **控制图像生成质量**

在Stable Diffusion中，改变去噪步骤是指调节模型在图像生成过程中的迭代步数。这些步骤构成了模型从含有随机噪声的初始状态逐渐生成目标图像的过程。其原理可以概述如下。
    - **初始状态**：模型起始于一个包含随机噪声的图像。
    - **逐步去噪**：在每个步骤中，模型依靠其经过训练的网络预测并减少图像中的噪声。 
    - **迭代次数**：整个过程涵盖了多个这样的去噪步骤。增加步骤数可以使模型有更多机会来细化图像，从而提高图像质量。 
    - **控制图像质量**：通过调整这些步骤的数量，可以精确控制生成图像的质量和细节水平。较多的步骤往往能产生更清晰、细节更丰富的图像，但也意味着需要更长的计算时间。

因此，通过改变去噪步骤的数量，用户可以在图像质量和生成速度之间找到一个合理的平衡点。我们可以通过调整下面代码的num_inference_steps值来控制图像的质量。
```python
prompt = "a sleeping cat enjoying the sunshine."
image = pipe(prompt, num_inference_steps=5).images[0]  # image here is in [PIL format]

# Now to display an image you can do either save it such as:
image.save(f"lovely_cat_sun.png")
image
```
不同num_inference_steps值生成的图像如图3-9和图3-10所示。 

![image](https://github.com/user-attachments/assets/8cce6373-c564-4655-8756-0eb1afc59d66)


4. **控制图像内容**

在Stable Diffusion中添加negative_prompt（负面提示）的作用是明确指出用户不希望在生成的图像中出现的元素或特征。这一做法对于模型而言，有助于更精确地识别并排除那些不被期望的内容，从而在生成图像时避免它们的出现。通过这样的方法，用户可以更细致地操控图像内容的生成，确保最终产出与个人的需求和偏好相符合。举例来说，如果用户想要创造一个不包含任何建筑物的自然风景画面，可以在负面提示中加入“无建筑”或类似的表述，以指导模型按照这一特定的指令进行创作。

可以通过如下代码来添加negative_prompt。
```python
prompt = "a sleeping cat enjoying the sunshine."
image = pipe(prompt, generator=generator,
             negative_prompt="tree and leaves").images[0]  # image here is in [PIL format]

# Now to display an image you can do either save it such as:
image.save(f"lovely_cat_sun_no_trees.png")
image
```
运行上述代码，输出如图3-11所示。

![image](https://github.com/user-attachments/assets/198e0405-8c48-4f16-b0bd-6baecf986621)


#### 3.2.3 可视化Stable Diffusion的内部工作机制

1. **前期准备**

为了能够提供可视化的展示，首先，为代码环境导入一些包。示例代码如下。

```python
!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)
!pip install -q mediapy
import itertools
import math
import mediapy as media
```
其次，为了能够观察到Stable Diffusion内部的工作原理，我们设计了两个回调函数。这些回调函数将会把过程中的图像保存到名为image_reservoir的列表中，同时将潜在特征保存到latents_reservoir中，方便之后查看。示例代码如下。
```python
!mkdir diffprocess
image_reservoir = []
latents_reservoir = []

@torch.no_grad()
def plot_show_callback(i, t, latents):
    latents_reservoir.append(latents.detach().cpu())
    image = pipe.vae.decode(1 / 0.18215 * latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).float().numpy()[0]
    # plt_show_image(image)
    plt.imsave(f"diffprocess/sample_{i:02d}.png", image)
    image_reservoir.append(image)

@torch.no_grad()
def save_latents(i, t, latents):
    latents_reservoir.append(latents.detach().cpu())
```
然后，生成一份图像，并将内部的演算过程逐步记录下来，供之后查看。示例代码如下。
```python
prompt = "a handsome cat dressed like Lincoln, trending art."
with torch.no_grad():
    image = pipe(prompt, callback=plot_show_callback).images[0]  # image here is in [PIL format]

# Now to display an image you can do either save it such as:
image.save(f"lovely_cat_lincoln.png")
image
```
运行上述代码，输出如图3-12所示。 

![image](https://github.com/user-attachments/assets/6d1a224e-a03c-4fb0-9952-e839c82242da)



2. **可视化图像序列**

接下来，可以通过以下代码查看之前保存的图像生成过程中的图像序列。

```python
media.show_video(image_reservoir, fps=5)
```

3. **可视化潜在特征**

首先查看潜在特征的张量形状。示例代码如下。
```python
latents_reservoir[0].shape
```

可以发现，由于潜在张量中有4个通道，因此，可以选择任意3个通道作为RGB进行可视化，并在Chan2RGB列表中放入0、1、2、3中的任何数字来选择通道，看看它们各自呈现出什么样的可视化效果。示例代码如下。
```python
Chan2RGB= [0,1,2]
latents_np_seq = [tsr[0,Chan2RGB].permute(1,2,0).numpy() for tsr in latents_reservoir]
media.show_video(latents_np_seq, fps=5)
```
通过观察生成的视频可以发现，在初始阶段，潜在特征充满随机性，与任何具体图像内容的关联非常弱。随着扩散步骤的进行，这些潜在特征逐渐被引导，开始形成更加结构化和具有目标图像特征的表示。

在每一步的去噪过程中，模型利用其训练中获得的知识来预测并减少潜在特征中的噪声，同时逐步引入与目标图像相关的细节。这个过程中，潜在特征从一种几乎无意义的噪声状态转变为能够表示清晰、具有图像内容的结构化数据。

#### 3.2.4 Stable Diffusion理论的实际应用
1. **简化版text2img函数**
本节将提供一个简化的采样函数版本，并建议在使用pipe(prompt)生成图像时探索这个函数的内部工作机制。通过在函数内输出张量并记录它们的形状，可以更深入地理解Stable Diffusion模型如何处理和生成图像。这种方法有助于揭示模型内部的具体操作，包括如何处理文本提示、生成初始噪声、应用UNet进行去噪，以及如何最终将潜在空间的数据解码为图像。通过这样的实验，可以加深对模型工作原理的认识。

具体流程如下。
    - **第1步，定义函数和参数**：函数generate_simplified接收文本提示（prompt）、负面提示（negative_prompt）、推理步数（num_inference_steps）和引导比例（guidance_scale）。
    - **第2步，文本嵌入**：使用模型的分词器和文本编码器（text_encoder）将文本提示转换为嵌入向量。 
    - **第3步，负面提示处理**：以类似方式处理负面提示，以生成嵌入向量。 
    - **第4步，初始化噪声**：生成初始的随机噪声（latent），作为图像生成的起点。 
    - **第5步，去噪过程**：这是整个算法的关键部分。在定义的步数中，模型逐步去除噪声。每一步使用UNet预测噪声，并应用分类器自由引导（classifier free guidance），这涉及将条件和无条件UNet的输出结合起来。具体的分解介绍如下。
        - UNet接收带有噪声的潜在空间（latent space）表示作为输入。
        - 逐步处理这些输入，使用其深层结构预测当前存在于图像中的噪声。 


### 3.2.4 Stable Diffusion理论的实际应用（续）

- **去除图像中的噪声**：应用分类器自由引导，结合条件预测和无条件预测，以引导图像更贴近文本提示。

- **更新潜在空间**：将去除噪声后的结果迭代至输入，更新潜在空间。

- **图像解码**：最终的潜变量被解码成图像，然后进行后处理以得到最终的图像输出。

示例代码如下：
```python
@torch.no_grad()
def generate_simplified(
    prompt = ["a lovely cat"],
    negative_prompt = ["",],
    num_inference_steps = 50,
    guidance_scale = 7.5,
    # do_classifier_free_guidance
    batch_size = 1,
    height, width = 512, 512,
    generator = None
):
    # get prompt text embeddings
    text_inputs = pipe.tokenizer(
        prompt,
        padding="max_length",
        max_length=pipe.tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    )
    text_input_ids = text_inputs.input_ids
    text_embeddings = pipe.text_encoder(text_input_ids.to(pipe.device))[0]
    bs_embed, seq_len, _ = text_embeddings.shape

    # get negative prompts text embedding
    max_length = text_input_ids.shape[-1]
    uncond_input = pipe.tokenizer(
        negative_prompt,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        return_tensors="pt",
    )
    uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(pipe.device))[0]

    # duplicate unconditional embeddings for each generation per prompt, using mps friendly method
    seq_len = uncond_embeddings.shape[1]
    uncond_embeddings = uncond_embeddings.repeat(batch_size, 1, 1)
    uncond_embeddings = uncond_embeddings.view(batch_size, seq_len, -1)

    # For classifier free guidance, we need to do two forward passes.
    # Here we concatenate the unconditional and text embeddings into a single batch
    # to avoid doing two forward passes
    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

    # get the initial random noise unless the user supplied it
    # Unlike in other pipelines, latents need to be generated in the target device
    # for 1-to-1 results reproducibility with the CompVis implementation.
    # However this currently doesn't work in'mps'.
    latents_shape = (batch_size, pipe.unet.in_channels, height // 8, width // 8)
    latents_dtype = text_embeddings.dtype
    latents = torch.randn(latents_shape, generator=generator, device=pipe.device, dtype=latents_dtype)

    # set timesteps
    pipe.scheduler.set_timesteps(num_inference_steps)
    # Some schedulers like PNDM have timesteps as arrays
    # It's more optimized to move all timesteps to correct device beforehand
    timesteps_tensor = pipe.scheduler.timesteps.to(pipe.device)

    # scale the initial noise by the standard deviation required by the scheduler
    latents = latents * pipe.scheduler.init_noise_sigma

    # Main diffusion process
    for i, t in enumerate(pipe.progress_bar(timesteps_tensor)):
        # expand the latents if we are doing classifier free guidance
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # predict the noise residual
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # perform guidance
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        # compute the previous noisy sample x_t -> x_t-1
        latents = pipe.scheduler.step(noise_pred, t, latents, ).prev_sample

    latents = 1 / 0.18215 * latents
    image = pipe.vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16
    image = image.cpu().permute(0, 2, 3, 1).float().numpy()
    return image

image = generate_simplified(
    prompt = ["a lovely cat"],
    negative_prompt = ["Sunshine",],)
plt_show_image(image[0])
```
运行上述代码，输出如图3-13所示。

![image](https://github.com/user-attachments/assets/063bb298-5188-4cc0-a2ec-e0f3fd94541c)


接下来将上述代码进行封装调用。示例代码如下：
```python
image = generate_simplified(
    prompt = ["a cat dressed like a ballerina"],
    negative_prompt = ["",],)
plt_show_image(image[0])
```
运行上述代码，输出如图3-14所示。

![image](https://github.com/user-attachments/assets/008082c1-93a7-4232-8c41-7e3fd1e562b3)


#### 2. 利用图像生成图像使用示例

首先导入需要的包，并创建模型。示例代码如下：

```python
from diffusers import StableDiffusionImg2ImgPipeline
device = "cuda"
model_path = "CompVis/stable-diffusion-v1-4"

pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
    model_path,
    variant="fp16", torch_dtype=torch.float16,
    use_auth_token=True
)
pipe = pipe.to(device)
```
```python
import requests
from io import BytesIO
from PIL import Image

url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
response = requests.get(url)
init_img = Image.open(BytesIO(response.content)).convert("RGB")
init_img = init_img.resize((768, 512))
print(type(init_img))
init_img
```
其次，获得一幅用于输入的初始图像，如图3-15所示。

![image](https://github.com/user-attachments/assets/e4c94998-64c1-4221-8931-1be377fff376)


最后，通过对Stable Diffusion进行提示输入，就可以在输入图像的基础上根据用户描述生成新的图像。
```python
prompt = "A fantasy landscape, trending on artstation"
generator = torch.Generator(device=device).manual_seed(1024)
with autocast("cuda"):
    image = pipe(prompt=prompt, init_image=init_img,
                 strength=0.75, guidance_scale=7.5,
                 generator=generator).images[0]
image
```

#### 3. 简化版img2img函数
由于这段代码的逻辑与之前提到的txt2img类似，因此这里主要介绍涉及的重点步骤。
- **第1步**：设置文本提示。定义文本提示和负面提示。
- **第2步**：编码文本提示。使用模型的分词器和文本编码器将文本提示转换为嵌入向量。
- **第3步**：初始化图像编码。如果初始图像（init_image）是PIL图像对象，则对其进行预处理并编码成潜在空间向量。
- **第4步**：添加噪声。根据模型的调度器（scheduler）和噪声生成器，向初始潜在向量添加噪声。
- **第5步**：迭代去噪过程。通过循环，模型逐步去除噪声，使用UNet来预测噪声残差，并根据文本嵌入进行分类器自由引导。
- **第6步**：解码图像。将最终的潜在空间向量解码成图像。

示例代码如下：
```python
@torch.no_grad()
def generate_img2img_simplified():
    prompt = ["A fantasy landscape, trending on artstation"]
    negative_prompt = [""]
    strength = 0.5 # strength of the image conditioning
    batch_size = 1
    num_inference_steps = 25
    init_image = init_img

    # set timesteps
    pipe.scheduler.set_timesteps(num_inference_steps)

    # get prompt text embeddings
    text_inputs = pipe.tokenizer(
        prompt,
        padding="max_length",
        max_length=pipe.tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    )
    text_input_ids = text_inputs.input_ids
    text_embeddings = pipe.text_encoder(text_input_ids.to(pipe.device))[0]

    # get unconditional embeddings for classifier free guidance
    uncond_tokens = negative_prompt
    max_length = text_input_ids.shape[-1]
    uncond_input = pipe.tokenizer(
        uncond_tokens,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        return_tensors="pt",
    )
    uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(pipe.device))[0]

    # For classifier free guidance, we need to do two forward passes.
    # Here we concatenate the unconditional and text embeddings into a single batch
    # to avoid doing two forward passes
    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

    # encode the init image into latents and scale the latents
    latents_dtype = text_embeddings.dtype
    if isinstance(init_image, PIL.Image.Image):
        init_image = init_preprocess(init_image)
        init_image = init_image.to(device=pipe.device, dtype=latents_dtype)
        init_latent_dist = pipe.vae.encode(init_image).latent_dist
        init_latents = 0.18215 * init_latent_dist.sample(generator=generator)

    # get the original timestep using init_timestep
    offset = pipe.scheduler.config.get("steps_offset", 0)
    init_timestep = int(num_inference_steps * strength) + offset
    init_timestep = min(init_timestep, num_inference_steps)

    timesteps = pipe.scheduler.timesteps
    timesteps = torch.tensor([timesteps[-init_timestep]] * batch_size, device=pipe.device)

    # add noise to latents using the timesteps
    noise = torch.randn(init_latents.shape, generator=generator, device=pipe.device, dtype=latents_dtype)
    init_latents = pipe.scheduler.add_noise(init_latents, noise, timesteps)
    latents = init_latents

    t_start = max(num_inference_steps - init_timestep + offset, 0)
    # Some schedulers like PNDM have timesteps as arrays
    # It's more optimized to move all timesteps to correct device beforehand
    timesteps = pipe.scheduler.timesteps[t_start:].to(pipe.device)

    for i, t in enumerate(pipe.progress_bar(timesteps)):
        # expand the latents if we are doing classifier free guidance
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # predict the noise residual
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # perform guidance
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        # compute the previous noisy sample x_t -> x_t-1
        latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample

    latents = 1 / 0.18215 * latents
    image = pipe.vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).numpy()
    return image
``` 
