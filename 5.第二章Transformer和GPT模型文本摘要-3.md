### 2.5 案例二：文本总结
在这个指南中，我们将一起探索文本摘要的技巧和秘密。想象一下，你有一大堆文档或文章，需要快速了解其要点，这时就可以使用文本摘要。文本摘要技术是NLP领域的一个重要分支，它涵盖了从长文本中提取关键信息的各种方法。这些方法大致分为两类：提取式摘要和生成式摘要。提取式摘要侧重于从原文中挑选出关键句子，而生成式摘要则重构信息，产生全新的文本内容。

#### 2.5.1 安装
我们建议你使用jupyternotebook来运行下面的代码（或者Google Colab）。你可能需要安装Hugging Face的transformers库和Datasets库以及其他依赖项。当然，也可以在Python环境中运行如下代码。
```Python
!pip install datasets evaluate transformers rouge-score nltk
```

为了能够与社区分享你的模型，并通过推理API生成图2 - 5所示的结果，你还需要遵循一些其他步骤。

![image](https://github.com/user-attachments/assets/1108c0cd-61a1-42d9-b9be-2a2785140277)


首先，从Hugging Face网站（如果还没有注册，请先注册）获取身份验证Token，然后执行以下代码并输入用户名和密码。
```Python
from huggingface_hub import notebook_login
notebook_login()
```

接下来，安装Git - LFS，之后在终端中运行以下命令。
```Shell
apt install git-lfs
```

确保transformers库版本至少为4.11.0。示例代码如下。
```Python
import transformers
print(transformers.__version__)
# 4.11.0
```

接下来我们将看到如何对Hugging Face的transformers库进行微调，以进行总结任务。我们将使用XSsum数据集（其中包含BBC文章和单句总结）在总结任务上进行模型训练。使用Hugging Face的Datasets库，我们能够非常简单地加载此任务的数据集；使用Hugging Face的框架下，我们还会使用Trainer API对模型进行微调。

本教程的代码可以加载Hugging Face的Model Hub中任何模型的checkpoint（检查点），只要该模型在transformers库中有对应的版本即可。这里将t5 - small checkpoint作为样例。
```Python
model_checkpoint = "t5-small"
```

#### 2.5.2 加载数据集
我们将使用Hugging Face的Datasets库下载数据并获取评估所需的度量标准（与基准模型进行比较）。这可以通过load_dataset或load_metric函数轻松完成这项任务。示例代码如下。
```Python
from datasets import load_dataset
from evaluate import load
raw_datasets = load_dataset('xsum')
metric = load('rouge')
```

运行上述代码，弹出如下警告信息。
```
/home/tiger/.local/lib/python3.9/site-packages/datasets/load.py:1429:
FutureWarning: The repository for xsum contains custom code which must be
executed to correctly load the dataset.
Passing `trust_remote_code=True` will be mandatory to load this dataset from
the next major release of `datasets`.
warnings.warn(...)
The dataset object itself is DatasetDict, which contains one key for the
training, validation and test set:
```

我们可以通过如下代码检查数据集里面的元素。
```Python
raw_datasets
```

运行上述代码，输出如下。
```
DatasetDict({
    train: Dataset({
        features: ['document','summary', 'id'],
        num_rows: 204045
    })
    validation: Dataset({
        features: ['document','summary', 'id'],
        num_rows: 11332
    })
    test: Dataset({
        features: ['document','summary', 'id'],
        num_rows: 11334
    })
})
```

可以发现数据集对象本身是DatasetDict，包含训练、验证和测试集3个键（key）。

为了访问实际元素，需要先选择raw_datasets的一个键，然后给出一个索引，例如1。示例代码如下。
```Python
raw_datasets["train"][1]
```

运行上述代码，输出如下。
```
{
    'document': 'A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\nAs they gathered outside they saw the two buses, parked side - by - side in the car park, engulfed in flames.\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\nPolice have appealed for information about the attack.\nInsp David Gibson said: "It appears as though the fire started under one of the buses before spreading to the second."\n"While the exact cause is still under investigation, it is thought that the fire was started deliberately."',
   'summary': 'Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.',
    'id': '40143035'
}
```

为了展示一些数据，我们可以使用如下函数随机显示数据集中的一些示例。
```Python
import datasets
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=5):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset) - 1)
        while pick in picks:
            pick = random.randint(0, len(dataset) - 1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))
```

训练之前我们还需要选择一个评测器，示例代码如下。
```Python
metric
```

运行上述代码，输出如下。
```
EvaluationModule(name: "rouge", module_type: "metric", features:
[{'predictions': Value(dtype='string', id='sequence'),'references': Sequence
(feature=Value(dtype='string', id='sequence'), length=-1, id=None)},
{'predictions': Value(dtype='string', id='sequence'),'references':
Value(dtype='string', id='sequence')}], usage: """
Calculates average rouge scores for a list of hypotheses and references
Args:
    predictions: list of predictions to score. Each prediction
        should be a string with tokens separated by spaces.
    references: list of reference for each prediction. Each
        reference should be a string with tokens separated by spaces.
    rouge_types: A list of rouge types to calculate.
        Valid names:
            "rouge{n}" (e.g. "rouge1", "rouge2") where: (n) is the n-gram
based scoring,
            "rougeL": Longest common subsequence based scoring.
            "rougeLsum": rougeLsum splits text using "
".
    See details in https://github.com/huggingface/datasets/issues/617
    use_stemmer: Bool indicating whether Porter stemmer should be used to strip
word suffixes.
    use_aggregator: Return aggregates if this is set to True
Returns:
    rouge1: rouge_1 (f1),
    rouge2: rouge_2 (f1),
    rougeL: rouge_1 (f1),
    rougeLsum: rouge_lsum (f1)
Examples:
    >>> rouge = evaluate.load('rouge')
    >>> predictions = ["hello there", "general kenobi"]
    >>> references = ["hello there", "general kenobi"]
    >>> results = rouge.compute(predictions=predictions, references=references)
    >>> print(results)
    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}
""", stored_examples: 0)
```

可以看到，此次使用的评测器是一种datasets.Metric。

我们可以调用metric对象的compute方法对模型的预测和数据集中的标签进行计算，以获得指标，示例代码如下。
```Python
fake_preds = ["hello there", "general kenobi"]
fake_labels = ["hello there", "general kenobi"]
metric.compute(predictions=fake_preds, references=fake_labels)
```

运行上述代码，输出如下。
```
{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}
```
### 2.5.3 预处理数据

接下来我们将上面的文本内容输入模型。在将这些文本输入到模型之前，需要对它们进行预处理，这个过程可以通过transformers库的tokenizer完成。正如名称分词器（tokenizer）所示，分词器将对输入的单词进行ID化，包括将单词转换为预训练词表中的相应ID，之后我们就能将这些代表单词的数字输入到模型中；随后模型也能生成ID，最后被转化成单词变为人们能够阅读的输出（单词）。

为了做到这一点，使用AutoTokenizer.from_pretrained方法实例化我们的分词器，这将确保：
- 得到的分词器与我们想要使用的模型架构相对应；
- 我们下载了在预训练此特定checkpoint时使用的词表。

由于该词表将被缓存，因此下次运行代码时不会再次下载。示例代码如下。

```js
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

默认情况下，上面的代码会调用来自transformers库的tokenizer快速分词器（由Rust支持）。你可以直接对一个句子或一对句子调用这个分词器。示例代码如下。

```js
tokenizer("Hello, this one sentence!")
```
### 2.5 案例二：文本总结
运行上述代码，输出如下。
```
{
    'input_ids': [8774, 6, 48, 80, 7142, 55, 1],
    'attention_mask': [1, 1, 1, 1, 1, 1, 1]
}
```
根据你选择的模型，上面的代码会返回不一样的ID。这部分细节对我们在这里所做的事情不太重要（只须知道它们是我们稍后将要实例化的模型所需的）。
我们可以传递一系列句子，而不是一个句子。示例代码如下。
```python
tokenizer(["Hello, this one sentence!", "This is another sentence."])
```
运行上述代码，输出如下。
```
{
    'input_ids': [
        [8774, 6, 48, 80, 7142, 55, 1],
        [100, 19, 430, 7142, 5, 1]
    ],
    'attention_mask': [
        [1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]
    ]
}
```
为了后续对tokenizer函数加入更多参数，所以使用text_target参数传入需要分词的句子数组，而不是像之前使用默认方式进行传参（而不使用text_target作为显式参数）。示例代码如下。
```python
print(tokenizer(text_target=["Hello, this one sentence!", "This is another sentence."]))
```
输出如下。
```
{
    'input_ids': [
        [8774, 6, 48, 80, 7142, 55, 1],
        [100, 19, 430, 7142, 5, 1]
    ],
    'attention_mask': [
        [1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1]
    ]
}
```
如果你正在使用T5的5个checkpoint（["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]）之一，我们必须在输入前加上“summarize: ”前缀（因为该模型也可以用于其他任务，比如翻译，所以需要前缀来表示它必须执行哪个任务）。示例代码如下。
```python
if model_checkpoint in ["t5-small", "t5-base", "t5-larg", "t5-3b", "t5-11b"]:
    prefix = "summarize: "
else:
    prefix = ""
```
然后我们可以编写预处理样本的函数。我们只须将它们喂给分词器，参数为`truncation=True`。这将确保如果输入的ID长度比“模型接受的最大长度”长，那么输入将被截断为“模型接受的最大长度”。稍后在数据整理器中将会运行填充逻辑（padding），填充逻辑会将ID填充到批次（batch）中的最长长度，而不是整个数据集最长长度。示例代码如下。
```python
max_input_length = 1024
max_target_length = 128

def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["document"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)
    # 处理目标分词器
    labels = tokenizer(text_target=examples["summary"], max_length=max_target_length, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs
```
preprocess_function函数适用于一个或多个示例。在多个示例的情况下，分词器将为每个输入返回一个列表的列表。示例代码如下。
```python
preprocess_function(raw_datasets['train'][:2])
```
运行上述代码，输出如下。
```
{
    'input_ids': [
        [21603, 10, 37, 423, 583, 13, 1783, 16, 20126, 16496, 6, 80, 13, 8, 844, 6025, 4161, 6, 19, 341, 271, 14841, 5, 7057, 161, 19, 4912, 16, 1626, 5981, 11, 186, 7540, 16, 1276, 15, 2296, 7, 5718, 2367, 14621, 4161, 57, 4125, 387, 5, 15059, 7, 30, 8, 4653, 4939, 711, 747, 522, 17879, 788, 12, 1783, 44, 8, 15763, 6029, 1813, 9, 7472, 5, 1404, 1623, 11, 5699, 277, 130, 4161, 57, 18368, 2016, 1216, 16496, 227, 8, 2473, 5895, 15, 147, 89, 22411, 139, 8, 1511, 5, 1485, 3271, 3, 21926, 9, 472, 19623, 5251, 8, 616, 12, 15614, 8, 1783, 5, 37, 13818, 10564, 15, 26, 3, 9, 19513, 1481, 6, 18368, 186, 1228, 2605, 30, 7488, 1887, 3, 18, 8, 711, 2309, 9517, 89, 355, 5, 3966, 1954, 9233, 15, 6, 1133, 2, 8, 7, 16548, 13363, 106, 14022, 84, 47, 14621, 4161, 6, 243, 255, 228, 59, 7828, 8, 1249, 18, 545, 11298, 1773, 728, 8, 8347, 1560, 5, 611, 6, 255, 243, 72, 1709, 1528, 161, 228, 43, 118, 4006, 91, 12, 766, 8, 3, 9513, 1481, 410, 59, 5124, 5, 96, 196, 17, 19, 1256, 68, 27, 103, 317, 132, 19, 78, 231, 23546, 21, 970, 51, 89, 2593, 11, 8, 2504, 189, 3, 18, 11, 27, 3536, 3653, 24, 3, 96, 18, 68, 34, 19, 966, 114, 62, 31, 60, 23708, 42, 3503, 147, 976, 255, 243, 5, 3, 96, 11880, 164, 59, 36, 1176, 68, 34, 19, 2361, 82, 4962, 147, 336, 360, 477, 5, 96, 17891, 130, 25, 59, 1065, 12, 199, 178, 3, 9, 720, 8, 72, 116, 8, 6337, 11, 8, 6196, 5685, 7, 141, 2767, 91, 4609, 7940, 6, 3, 9, 8347, 5685, 3048, 16, 286, 640, 8, 17600, 7, 250, 13, 8, 3917, 3412, 5, 1276, 15, 2296, 7, 47, 14621, 1560, 7, 982, 6, 65, 53, 3088, 12, 4277, 72, 13613, 7, 16, 8, 616, 5, 12580, 17600, 7, 2063, 65, 1233, 3, 9, 570, 30, 165, 475, 13, 8, 7540, 6025, 4161, 11, 3863, 43, 118, 3, 19492, 59, 12, 9751, 12493, 3957, 5, 37, 16117, 3450, 31, 7, 21108, 12580, 2488, 5104, 11768, 1306, 47, 16, 1626, 5981, 30, 2089, 12, 217, 8, 1419, 166, 609, 5, 216, 243, 34, 47, 359, 12, 1229, 8, 8347, 1711, 515, 269, 68, 3, 9485, 3088, 12, 1634, 95, 8, 433, 5, 96, 196, 47, 882, 1026, 3, 9, 1549, 57, 8, 866, 13, 1783, 24, 65, 118, 612, 976, 3, 88, 243, 5, 96, 14116, 34, 19, 842, 18, 18087, 21, 151, 113, 43, 118, 5241, 91, 13, 70, 2503, 11, 8, 1113, 30, 1623, 555, 216, 243, 34, 47, 359, 24, 96, 603, 5700, 342, 2245, 121, 130, 1026, 12, 1822, 8, 844, 167, 9930, 11, 3, 9, 964, 97, 3869, 474, 16, 286, 21, 8347, 9793, 1390, 5, 2114, 25, 118, 4161, 57, 18368, 16, 970, 51, 89, 2593, 11, 10987, 32, 1343, 227, 18368, 2593, 57, 16133, 4937, 5, 11],
        [21603, 10, 71, 1472, 6196, 877, 326, 44, 8, 9108, 86, 29, 16, 6000, 1887, 44, 81, 11484, 10, 1755, 272, 4209, 30, 1856, 11, 2554, 130, 1380, 12, 1175, 8, 1963, 5, 282, 79, 3, 9094, 1067, 79, 1509, 8, 192, 14264, 6, 3, 1669, 596, 18, 8, 18, 1583, 16, 8, 443, 2447, 6, 3, 35, 6106, 19565, 57, 12314, 5, 455, 13, 164, 1552, 1637, 19, 45, 3434, 6, 8, 119, 45, 1473, 11, 14441, 5, 8, 47, 70, 18, 1006, 5616, 5, 37, 25335, 13, 80, 13, 8, 4264, 243, 73, 16, 8, 9234, 4161, 646, 525, 12770, 7, 30, 1476, 11, 175, 141, 118, 10932, 5, 16, 8, 16, 8, 706, 5961, 5376, 57, 56, 1731, 70, 1552, 13, 8, 3457, 4393, 2367, 1637, 43, 3666, 3709, 11210, 3588, 15, 26, 21, 251, 81, 8, 3211, 5, 866, 75, 145, 79, 141, 4335, 5076, 43, 17, 3475, 3
```

如果要将此函数应用于我们之前创建的数据集对象中的所有句子对，我们只须使用dataset对象的map方法。这将在数据集所有分割中的所有元素上应用该函数，因此我们的训练、验证和测试数据将通过这一条命令完成预处理。示例代码如下。 （此处未给出具体示例代码 ） 




