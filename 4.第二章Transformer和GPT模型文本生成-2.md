### 2.4 案例一：文本生成

主流的语言建模主要有两种方案——因果（causal）和遮蔽（masked）。本书演示了因果语言建模。因果语言模型经常用于文本生成。你可以将这些模型用于创造性应用，如选择自己的文本冒险或智能编码助手，如Copilot或CodeParrot。



因果语言模型预测一系列令牌（Token）中的下一个Token，模型只能关注左侧的Token。这意味着模型不能看到未来的Token。GPT-2是因果语言模型的一个例子。


你可以按照本书中的相同步骤对其他架构进行因果语言建模微调。可选择的架构如下。
```Python
BART、BERT、Bert_Generation、BigBird、BigBird-Pegasus、BioGpt、Blenderbot、BlenderbotSmall、BLOOM、CamemBERT、CodeLlama、CodeGen、CFM-Ant、CTRL、Data2VecText、ELECTRA、ERNIE、Falcon、FuyuBERT、GPT-Sw3、OpenAI_GPT-2、GPTBigCode、GPT_Neo、GPT_NeoX、GPT_NeoX_日语、GPT-J、LLaMA、Marian、mBART、MEGA、Megatron-BERT、Mistral、Mixtral、MPT、MusicGen、MVP、OpenLlama、OpenAI_GPT、OPT、Pegasus、Persimmon、Phi、PI.Bert、ProphetNet、QDQN、Reformer、RemBERT、RoBERTa、RoBERTa-PreLayerNorm、RoCBert、RoFormer、RWKV、Speech2Text2、Transformer-XL、TrOCR、Whisper、XGLM、XLM、XLM-ProphetNet、XLM-RoBERTa、XLM-RoBERTa-XL、XLNet、X-MOD
```

#### 2.4.1 初始化

在开始之前，请确保你安装了所有必要的库。示例代码如下。

```Python

pip install transformers datasets evaluate
```

我们鼓励读者登录到Hugging Face账户，以便可以上传并与社区共享模型。在出现相关提示时，输入你的Token以登录。示例代码如下。

```Python
from huggingface_hub import notebook_login
notebook_login()
```

#### 2.4.2 加载ELI5数据集
首先从数据集库加载r/askscience子集的ELI5数据集的一个较小子集。这将让你有机会进行实验，确保一切正常，然后再花费更多时间在完整数据集上进行训练。示例代码如下。
```Python
from datasets import load_dataset
eli5 = load_dataset("eli5", split="train_asks[:5000]")
```

使用train_test_split方法将数据集的train_asks分割成训练集和测试集。示例代码如下。
```Python
eli5 = eli5.train_test_split(test_size=0.2)
```

一个示例如下。
```Python
eli5["train"][0]
{
    'answers': {
        'a_id': ['c3d1aib', 'c3d41ya'],
       'score': [6, 3]
    },
    'text': [
        "The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
        "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"
    ],
    'answers_urls': {'url': []},
    'document': '',
    'q_id': 'nyxfp',
   'selftext': 'URL_0\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
   'selftext_urls': {'url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg']},
   'subreddit': 'askscience',
    'title': 'Few questions about this space walk photograph.',
    'title_urls': {'url': []}
}
```

虽然这看起来内容很多，但你实际上只对文本字段感兴趣。语言建模任务的有趣之处在于你不需要标签（也称为无监督任务），因为模型输入的下一个“词”就是标签。

#### 2.4.3 预处理
下一步是加载DistilGPT2分词器来处理文本子字段。示例代码如下。
```Python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
```

从上面的代码可以看出，文本字段实际上嵌套在answers内。这意味着你需要使用flatten方法将文本子字段从其嵌套结构中提取出来。示例代码如下。
```Python
eli5 = eli5.flatten()
>>> eli5["train"][0]
{
    'answers.a_id': ['c3d1aib', 'c3d41ya'],
    'answers.score': [6, 3],
    'answers.text': [
        "The velocity needed to remain in orbit is equal to the square root of Newton's constant times the mass of earth divided by the distance from the center of the earth. I don't know the altitude of that specific mission, but they're usually around 300 km. That means he's going 7-8 km/s.\n\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.",
        "Hope you don't mind me asking another question, but why aren't there any stars visible in this photo?"
    ],
    'answers_urls.url': [],
    'document': '',
    'q_id': 'nyxfp',
   'selftext': 'URL_0\n\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?',
   'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
   'subreddit': 'askscience',
    'title': 'Few questions about this space walk photograph.',
    'title_urls.url': []
}
```

```js
'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
'subreddit': 'askscience',
'title': 'Few questions about this space walk photograph.',
'title_urls.url': []
}

eli5 = eli5.flatten()
eli5["train"][0]
{
    'answers.a_id': ['c3d1aib', 'c3d41ya'],
    'answers.score': [6, 3],
    'answers.text': [
        "为了保持在轨道上，所需的速度等于牛顿常数的平方根乘以地球质量除以距地球中心的距离。我不知道那个特定任务的高度，但它们通常在300km左右。这意味着他的速度是7 - 8km/s。\n\n在太空中，没有其他力作用在航天飞机或那个人身上，所以他们保持相对于彼此的同一位置。如果他无法返回飞船，他可能会耗尽氧气，或慢慢降入大气层并燃烧。",
        "希望你不介意我再问一个问题，但为什么这张照片中看不到任何星星？"
    ],
    'answers_urls.url': [],
    'document': '',
    'q_id': 'nyxfp',
   'selftext': 'URL_0\n\n这是今天早些时候的头条新闻，我对此有几个问题。是否有可能计算出宇航员将以多快的速度绕地球飞行？他是如何靠近航天飞机的，以便能安全返回，即他是以相同的速度绕行，因此可以留在旁边？最后，如果他的推进系统失效，他是否最终会重新进入大气层并可能死亡？',
   'selftext_urls.url': ['http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg'],
   'subreddit': 'askscience',
    'title': '关于这张太空行走照片的几个问题。',
    'title_urls.url': []
}
```
现在每个子字段都是一个单独的列，如answers前缀所示，文本字段现在是一个列表。不要分别对每个句子进行分词，而是将列表转换为字符串，以便你可以对它们进行联合分词。

如下代码展示了第一步预处理函数，用于连接每个示例的字符串列表并对结果进行分词。
```Python
def preprocess_function(examples):
    return tokenizer([" ".join(x) for x in examples["answers.text"]])
```

要在整个数据集上应用此预处理函数，请使用Hugging Face数据集的map方法。你可以通过设置batched=True来加速map函数，一次处理多个数据集元素，并增加处理数目（num_proc）。删除你不需要的任何列。示例代码如下。
```Python
tokenized_eli5 = eli5.map(
    preprocess_function,
    batched=True,
    num_proc=4,
    remove_columns=eli5["train"].column_names,
)
``` 
这个数据集包含了Token序列，但其中一些比模型的最大输入长度更长。
你现在可以使用第2个预处理函数来连接所有序列，并将连接的序列分割成由block_size定义的较短块。其中，block_size既要短于模型的最大输入长度，又要短于你的GPU RAM（显存）。示例代码如下。
```Python
block_size = 128

def group_texts(examples):
    # Concatenate all texts.
    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
    total_length = len(concatenated_examples[list(examples.keys())[0]])
    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can
    # customize this part to your needs.
    if total_length >= block_size:
        total_length = (total_length // block_size) * block_size
    # Split by chunks of block_size.
    result = {
        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
        for k, t in concatenated_examples.items()
    }
    result["labels"] = result["input_ids"].copy()
    return result
```

在整个数据集上应用group_texts函数。示例代码如下。
```Python
lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)
```

现在使用DataCollatorForLanguageModeling创建一批示例。在批量处理过程中，对每个批次中最长的句子进行动态填充，这比将整个数据集填充至最大长度更为高效。
使用序列结束标记作为填充Token并设置mlm=False。这将使用输入作为右移一个元素的标签。示例代码如下。
```Python
from transformers import DataCollatorForLanguageModeling
tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
```

### 2.4.4 训练
现在可以开始训练你的模型了！使用AutoModelForCausalLM加载DistilGPT2。示例代码如下。
```Python
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
model = AutoModelForCausalLM.from_pretrained("distilgpt2")
```

此时，只剩下如下3个步骤。
第1步：在TrainingArguments中定义训练超参数。唯一需要的参数是output_dir，它指定了保存模型的位置。你可以通过设置push_to_hub=True，将此模型推送到Hub（需要登录到Hugging Face才能上传模型）。
第2步：将训练参数传递给Trainer，以及模型、数据集和数据整理器。
第3步：调用train方法来微调你的模型。
示例代码如下。
```Python
training_args = TrainingArguments(
    output_dir="my_awesome_eli5_clm-model",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    weight_decay=0.01,
    push_to_hub=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset["train"],
    eval_dataset=lm_dataset["test"],
    data_collator=data_collator,
)

trainer.train()
```

训练完成后，使用evaluate方法评估你的模型并获取其困惑度（Perplexity）。示例代码如下。
```Python
import math

eval_results = trainer.evaluate()
print(f"Perplexity: {math.exp(eval_results['eval_loss']):.2f}")
```

然后使用push_to_hub方法将模型分享到Hub，这样每个人都可以使用你的模型。示例代码如下。
```Python
trainer.push_to_hub()
```

### 2.4.5 推理
现在你已经微调了一个模型，可以用它来进行推断！

想出一个你想从中生成文本的提示。示例代码如下。
```Python
prompt = "Somatic hypermutation allows the immune system to"
# prompt = "体细胞超变异允许免疫系统"
```

当我们想尝试使用微调模型进行推理时，最简单的方法是在pipeline方法中使用它。使用你的模型实例化一个文本生成pipeline，并将你的文本传递给它。示例代码如下。
```Python
from transformers import pipeline

generator = pipeline("text-generation", model="my_awesome_eli5_clm-model")
generator(prompt)
# [{'generated_text': 'Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\n\nThe damage caused by an infection is caused by the immune system\'s ability to perform its own self-correcting tasks.'}]
# [{'generated_text': '体细胞超变异允许免疫系统能够有效地逆转感染造成的损害。\n\n感染造成的损害是由免疫系统执行其自我修正任务的能力造成的。'}]
```

对文本进行分词并将input_ids作为PyTorch张量返回。示例代码如下。
```Python
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("my_awesome_eli5_clm-model")
inputs = tokenizer(prompt, return_tensors="pt").input_ids
```

使用generate方法生成文本。示例代码如下。
```Python
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("my_awesome_eli5_clm-model")
outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
```

将生成的Token id解码回文本。示例代码如下。
```Python
tokenizer.batch_decode(outputs, skip_special_tokens=True)
# ["Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system"]
# ["体细胞超变异允许免疫系统对药物产生反应，具有适应不同环境情况的能力。换句话说，'超变异'系统可以帮助免疫系统适应不同的环境情况，甚至在某些情况下适应单一生命。相比之下，马萨诸塞大学波士顿分校的研究人员发现，'超变异'在小鼠中比在人类中更强，但在人类中也可以发现，这对免疫系统来说并不是完全未知的。关于免疫系统如何"]
```


