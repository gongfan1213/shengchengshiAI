### 5.3 使用Transformer处理多模态大语言模型
Qwen-VL是阿里云研发的大规模视觉语言模型（Large Vision Language Model, LVL）。Qwen-VL可以以图像、文本、检测框作为输入，并以文本和检测框作为输出。Qwen-VL系列模型的特点如下。
- 强大的性能。在四大类多模态任务的标准英文测评中（如Zero-shot Captioning、VQA、DocVQA、Grounding），均取得同等通用模型大小情况下的最好效果。
- 多语言对话模型。天然支持英文、中文等多语言对话，端到端支持图像里中英双语的长文本识别。
- 多图交错对话。支持多图输入和比较、指定图像问答、多图文学创作等。
- 首个支持中文开放域定位的通用模型。通过中文开放域语言表达进行检测框标注。
- 细粒度识别和理解。相比于目前其他开源LVLM使用的224分辨率，Qwen-VL是首个开源的448分辨率的LVLM。提高分辨率可以提升细粒度的文字识别、文档问答和检测框标注。


  #### 5.3.1 配置教程
具体的配置步骤如下。
- **第1步**：拉取Qwen-VL项目的仓库至本地。示例代码如下。
### Shell
```bash
git clone https://github.com/QwenLM/Qwen-VL.git
```
- **第2步**：创建项目所属的虚拟环境。本项目的环境要求为Python 3.8及以上，PyTorch 1.12及以上版本，推荐2.0及以上版本，CUDA 11.4及以上（GPU用户需考虑此选项）。示例代码如下。
### Shell
```bash
conda create -n qwenvl python=3.8
conda activate qwenvl
pip install torch==1.13.0+cu116 torchvision==0.14.0+cu116 torchaudio==0.13.0 --extra-index-url https://download.pytorch.org/whl/cu116
```
- **第3步**：配置项目所需环境。由于Qwen-VL项目已经将所需配置列入requirements.txt文件中，因此用户可以使用以下代码快速配置。
### Shell
```bash
cd Qwen-VL
pip install -r requirements.txt
```
- **第4步**：采用预训练模型。为了采用已经训练好的预训练模型进行测试，用户需要连接Hugging Face，再从其中下载Qwen-VL的预训练模型，相关的示例代码会在5.3.2节展示。但是，一些用户可能会遇到无法连接到Hugging Face的情况，这可能是因为urllib3库的版本升级后，识别“https”前缀网址的部分不能兼容。因此，可以使用如下代码降低urllib3库的等级，以正确连接Hugging Face。
### Shell
```bash
pip install urllib3==1.25.11
```
#### 5.3.2 使用Transformer
### 1. 运行示例
使用Transformer的示例代码如下。
### Python
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch
torch.manual_seed(1234)

# Note: The default behavior now has injection attack prevention off
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# use bf16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
# use fp16
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
# use cpu only
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cpu", trust_remote_code=True).eval()
# use cuda device
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()

# Specify hyperparameters for generation
model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-VL-Chat", trust_remote_code=True)

# 1st dialogue turn
query = tokenizer.from_list_format([
    {'image': './assets/imgs/picture.jpg', # Either a local path or an url
     'text': '描述画面内容。'}
])
response, history = model.chat(tokenizer, query=query, history=None)
print(response)
# 图中是两个美术生画的同一个人物，画面上是一个身穿红色衣服的女学生坐在椅子上。左边的画面中，女学生穿着红色衣服、黑色裤子、红色鞋子，坐在靠墙的椅子上，将身体靠在椅背上，将左手放在椅子上，右手放在身边。右边的画面是该学生坐在椅子上的线稿图

# 2nd dialogue turn
response, history = model.chat(tokenizer, '框出图中的真人', history=history)
print(response)
# <ref>真人</ref><box>(21,29),(526,837)</box>
image = tokenizer.draw_bbox_on_latest_picture(response, history)
if image:
    image.save('1.jpg')
else:
    print("no box")
```
运行上述代码，输出的注释如下。
### Python
```python
# 图中是两个美术生画的同一个人物，画面上是一个身穿红色衣服的女学生坐在椅子上。左边的画面中，女学生穿着红色衣服、黑色裤子、红色鞋子，坐在靠墙的椅子上，将身体靠在椅背上，将左手放在椅子上，右手放在身边。右边的画面是该学生坐在椅子上的线稿图
# <ref>真人</ref><box>(21,29),(526,837)</box>
```
示例代码中使用的初始图像如图5-11所示，生成的图像如图5-12所示。 （此处未详细描述图5-11、图5-12内容 ） 

![image](https://github.com/user-attachments/assets/fc22d470-477f-4555-b2c7-8671a37488fa)


可以看到，Qwen-VL可以很好地理解中文问题，并对提出的问题做出正确、合理的回答。
### 2. 3种不同的预训练模型
在大语言模型领域，预训练模型扮演着至关重要的角色。一旦模型在预训练阶段学习了语言的基本规则和模式，它就可以通过少量的任务特定数据进行微调，以适应特定的应用场景。这种方法极大地提高了模型的适用性和灵活性，减少了对大量标记数据的依赖，同时也降低了训练成本。

在上面给出的运行示例中，可以看到一个函数名称多次出现，那就是from_pretrained。在代码段中，from_pretrained函数的作用是为当前程序导入预训练模型，供使用者调用。虽然它们的名字一样，但它们却是3个不同的函数，接下来分析每个from_pretrained函数的具体作用。
- **1）分词器**：第1次出现时，使用AutoTokenizer.from_pretrained方法加载一个预训练好的分词器，其作用是初始化并加载与Qwen/Qwen-VL-Chat模型相关联的分词器，它将用于文本的预处理步骤，确保输入格式与模型训练时的格式一致。这是准备模型进行推理或继续训练前的一个关键步骤。函数的参数列表如下。
### Python
```python
def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):
```
可以看到，必须输入的参数就是需要加载的预训练模型名称或者路径。当然，函数也准备了其他可选参数，便于用户灵活地调整导入方式，如cache_dir（缓存目录位置）、force_download（是否强制重新下载）、resume_download（是否删除未完全下载的安装包）等。如果遇到网络不佳或下载中断的情况，可以尝试调整这些参数来避免资源的重复下载。
- **2）预训练模型**：第2次出现时，使用AutoModelForCausalLM.from_pretrained方法加载了一个预训练的因果语言模型，并通过.eval函数将其置于评估模式，这样就可以使用该模型进行文本生成或者其他评估任务。这里的model是一个模型对象，它包含了网络结构和预训练权重，但直到这一步，并没有配置模型生成文本时的行为。函数的参数列表如下。
### Python
```python
def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):
    config = kwargs.pop("config", None)
    trust_remote_code = kwargs.pop("trust_remote_code", None)
    kwargs["_from_auto"] = True
    hub_kwargs_names = [
        "cache_dir",
        "code_revision",
        "force_download",
        "local_files_only",
        "proxies",
        "resume_download",
        "revision",
        "subfolder",
        "use_auth_token",
        "token",
    ]
```
- **3）模型配置文件**：第3次出现时，使用GenerationConfig.from_pretrained加载了与模型相关的文本生成配置。GenerationConfig可能包含了如生成文本的最大长度、多样性参数、温度等控制生成过程的参数。通过将这个配置对象赋值给model.generation_config属性，可以定制模型生成文本的具体行为。函数的参数列表如下。
### Python
```python
def from_pretrained(
    cls,
    pretrained_model_name: Union[str, os.PathLike],
    config_file_name: Optional[Union[str, os.PathLike]] = None,
    cache_dir: Optional[Union[str, os.PathLike]] = None,
    force_download: bool = False,
    local_files_only: bool = False,
    token: Optional[Union[str, bool]] = None,
    revision: str = "main",
    **kwargs,
) -> "GenerationConfig":
```
- **4）分离加载的好处**：这种分离配置的设计允许用户在不改变模型本身的情况下灵活地调整生成文本的行为，也能根据不同的应用场景灵活地调整分词策略。这样，如果用户想要尝试不同的生成策略，只须更改generation_config、AutoTokenizer，而不是重新加载整个模型。这提供了更快速的实验迭代，因为配置通常远小于模型本身，更改配置并不涉及复杂的计算。
- **5）加载的基本流程**：当然，通过阅读函数实现的具体方式可以发现，各种不同的加载函数，其中的主要逻辑都是相似的。下面通过其中一个from_pretrained函数来简单说明。具体操作步骤如下。
  - **第1步**：处理授权Token。这一步是为了处理有授权Token的情况，兼容不同的设备和使用场景，采用了多重判断来兼容不同版本的参数传递方式。示例代码如下。
### Python
```python
token = hub_kwargs.pop("token", None)
use_auth_token = hub_kwargs.pop("use_auth_token", None)
if use_auth_token is not None:
    warnings.warn(
        "The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.", FutureWarning
    )
    if token is not None:
        raise ValueError(
            "`token` and `use_auth_token` are both specified. Please set only the `token`."
        )
    token = use_auth_token
if token is not None:
    hub_kwargs["token"] = token
```
  - **第2步**：处理配置对象和配置参数。这一步判断是否提供了配置对象，如果没有，则从预训练模型名称或路径中加载配置。这个过程会调用AutoConfig.from_pretrained方法，并根据配置对象获取模型类型和相关参数。示例代码如下。
### Python
```python
if not isinstance(config, PretrainedConfig):
    kwargs_orig = copy.deepcopy(kwargs)
    # ensure not to pollute the config object with torch_dtype="auto" - since it's
    # meaningless in the context of the config object - torch.dtype values are acceptable
    if kwargs.get("torch_dtype", None) == "auto":
        _ = kwargs.pop("torch_dtype")
    # to not overwrite the quantization_config if config has a quantization_config
    if kwargs.get("quantization_config", None) is not None:
        _ = kwargs.pop("quantization_config")
    config, kwargs = AutoConfig.from_pretrained(
        pretrained_model_name_or_path,
        return_unused_kwargs=True,
        trust_remote_code=trust_remote_code,
        **hub_kwargs,
        **kwargs,
    )
    # if torch_dtype=auto was passed here, ensure to pass it on
    if kwargs_orig.get("torch_dtype", None) == "auto":
        kwargs["torch_dtype"] = "auto"
    if kwargs_orig.get("quantization_config", None) is not None:
        kwargs["quantization_config"] = kwargs_orig["quantization_config"]
```
  - **第3步**：确定模型加载方式。这一步是确定模型加载的方式，即是否通过Hub远程加载模型或者本地加载。根据配置对象的属性和参数，以及用户是否信任远程代码，来决定加载模型的方式。示例代码如下。
### Python
```python
has_remote_code = hasattr(config, "auto_map") and cls.__name__ in config.auto_map
has_local_code = type(config) in cls.model_mapping.keys()
trust_remote_code = resolve_trust_remote_code(
    trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code
)
```
  - **第4步**：加载模型类并返回实例。这是实现功能的最重要的步骤，根据前面步骤的结果，确定最终加载模型的类别和参数，并返回实例化的模型对象。具体来说，如果有远程代码并且信任远程代码，从动态模块加载模型类并返回实例；如果配置对象的类型在模型映射字典中，则直接从模型映射字典中获取模型类并返回实例。这样的判断也保证了能够符合绝大多数的模型存储模式。示例代码如下。
### Python
```python
if has_remote_code and trust_remote_code:
    class_ref = config.auto_map[cls.__name__]
    model_class = get_class_from_dynamic_module(
        class_ref, pretrained_model_name_or_path, **hub_kwargs, **kwargs
    )
    _ = hub_kwargs.pop("code_revision", None)
    if os.path.isdir(pretrained_model_name_or_path):
        model_class.register_for_auto_class(cls.__name__)
    else:
        cls.register(config.__class__, model_class, exist_ok=True)
    return model_class.from_pretrained(
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    )
elif type(config) in cls.model_mapping.keys():
    model_class = _get_model_class(config, cls.model_mapping)
    return model_class.from_pretrained(
        pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    )
raise ValueError(
    f"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n"
    f"Model type should be one of {' ,'.join(c.__name__ for c in cls.model_mapping.keys())}."
)
```
### 3. 使用fp16和bf16
#### 1）理论介绍
在大型模型训练过程中，bf16和fp16分别代表两种不同的数值精度格式。fp16（半精度浮点数）使用16位来表示浮点数，其中1位用于符号，5位用于指数，10位用于尾数。bf16(brain floating point）是一种16位的浮点格式，由Google提出，用1位表示符号，8位表示指数，7位表示尾数，它解决了fp16的数据范围较小的问题。它们的计算方式参考图5-13和图5-14，相关原理如图5-15和图5-16所示。 （此处未详细描述图5-13 - 图5-16内容 ） 


![image](https://github.com/user-attachments/assets/c044aa84-f7bc-4cf9-b899-0ee0fb1c937e)


从上述的原理介绍来看，采用bf16和fp16浮点格式在深度学习和高性能计算领域中，实现了对计算资源优化的有效策略。这两种数据表示形式通过减少浮点数所需的位宽，从而降低了对存储空间和内存带宽的需求。这种减少不仅显著提高了数据处理的效率，也优化了能源消耗，对于在有限的硬件资源条件下实现大规模数据处理尤为重要。

尽管fp16和bf16格式在节约内存、加速计算等方面为深度学习和高性能计算带来显著优势，但是它们也伴随着一些不可忽视的缺点。最主要的问题是精度损失：fp16和bf16由于位宽减半，无法像fp32那样细致地表示浮点数，这在精度敏感的应用程序中可能导致结果的不准确或波动。此外，虽然这些格式可以显著加速计算，但并非所有硬件和软件框架都支持fp16和bf16，这限制了它们的通用性。在没有专门优化的系统中，使用这些格式可能不会带来预期的性能提升。再者，对于某些特定的数值计算问题，如累积误差较大的场景，fp16和bf16的使用需要更加谨慎，以避免误差的放大。因此，在选择使用这些浮点格式时，必须综合考虑应用场景的具体需求和可接受的精度损失，以确保既能充分利用其优势，又能避免潜在的问题。

通常fp16精度支持V100、P100、T4等显卡，bf16则支持A100、H100、RTX3060、RTX3070等显卡。总的来说，bf16是英伟达公司更加先进的混合精度方案。
#### 2）代码演示
将前面给出的demo示例代码稍做修改，就可以查看效果。具体来说，这里使用fp16的数据格式进行演示。可以先注释掉如下代码。
### Python
```python
# model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="cuda", trust_remote_code=True).eval()
```
同时修改为使用fp16格式的调用语句。
### Python
```python
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-VL-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
```
其余代码都不需要修改。再次运行代码，可以得到图5-17所示的图像。 （此处未详细描述图5-17内容 ） 

![image](https://github.com/user-attachments/assets/a7eeb46e-88db-425a-8248-5e540a3dd46e)


从输出结果可以看出，使用fp16的数据精度和正常的数据精度相比，回答结果的大致内容是相似的，这也印证之前提到的，只要合理运用fp16的数据精度，就不会引起错误的结果，反而可以提高运算速度，减小运算内存。但是观察两个回答的区别，可以发现，使用正常数据精度的回答更加细致和贴切。因此，在实际训练大语言模型时，要结合硬件性能和训练精细程度，合理使用如f16,bf16等减少运算复杂程度的操作

### 4.对话功能

在实例代码当中，核心功能就是实现对话的交互，也就是代码当中的model.chat函数，接下来结合代码来分析chat的具体实现。
