```python
import torch
from diffusers import AutoPipelineForText2Image

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
image = pipeline(
    "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
guidance_scale=3.5
).images[0]
image
```
图3 - 41~图3 - 43展示了guidance_scale分别为3.5、5和10时模型生成的图像。
- 图3 - 41 guidance_scale = 3.5时模型生成的图像
- 图3 - 42 guidance_scale = 5时模型生成的图像
- 图3 - 43 guidance_scale = 10时模型生成的图像


![image](https://github.com/user-attachments/assets/1daa87c5-1422-44af-84c7-8ed9bd2faf62)


### 3. negative_prompt参数
更多关于negative_prompt参数的信息，请参见3.4.2节。negative_prompt参数通常用于通过移除如“低分辨率”或“细节差”等不良图像特征来提高整体图像质量。如果输入其他负面提示，如丛林“Jungle”，则发现图像中不会出现丛林。我们还可以使用negative_prompt来移除或修改图像的内容和风格。我们可以从diffusers库中导入AutoPipelineForImage2Image函数来实现此功能。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=negative_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```

#### 3.5.4 链式图像到图像pipeline
除了生成图像以外，可以通过其他有趣的方式使用图像到图像pipeline，还可以更进一步，将其与其他pipeline串联。
### 1. 从文本到图像，再从图像到图像
将文本到图像和图像到图像的pipeline进行串联，可以让你从文本生成图像，然后使用生成的图像作为图像到图像pipeline的初始图像。这在你想要完全从头开始生成图像时非常有用。例如，让我们将稳定扩散模型和康定斯基模型串联起来。

首先，使用文本到图像pipeline生成一幅图像。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image
from diffusers.utils import make_image_grid

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

text2image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k").images[0]
text2image
```
然后，将生成的图像传递给图像到图像pipeline。示例代码如下。
```python
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image2image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k", image=text2image).images[0]
make_image_grid([text2image, image2image], rows=1, cols=2)
```
### 2. 图像到图像串联
可以将多个图像到图像pipeline串联在一起，以创建更有趣的图像。这对于迭代地在图像上进行风格转换、生成短GIF、恢复图像的颜色，或修复图像的缺失区域非常有用。

首先，通过如下代码生成一幅图像。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, output_type="latent").images[0]
```
**小提示**：在pipeline中指定output_type="latent"很重要，可以保持所有输出在潜在空间中，避免不必要的解码 - 编码步骤。这只适用于使用相同VAE的串联pipeline。

然后，将这个pipeline的潜在输出传递给下一个pipeline，以生成一幅漫画风格的图像。示例代码如下。
```python
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "ogkalu/Comic-Diffusion", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
#pipeline.enable_xformers_memory_efficient_attention()

# need to include the token "charliebo artstyle" in the prompt to use this checkpoint
image = pipeline("Astronaut in a jungle, charliebo artstyle", image=image, output_type="latent").images[0]
```
运行上述代码，初始图像与生成的动漫风格的图像的对比如图3 - 44所示。

![图3-44 初始图像与生成的动漫风格的图像的对比](此处无实际图片内容，按原文描述应是初始图像与生成的动漫风格的图像的对比相关图像)

![image](https://github.com/user-attachments/assets/1ce9c6fa-e32b-4956-a930-d385a20fa76d)


重复一次上述过程，以生成最终的像素艺术风格图像。示例代码如下。
```python
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kohbanye/pixel-art-style", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
#pipeline.enable_xformers_memory_efficient_attention()

# need to include the token "pixelartstyle" in the prompt to use this checkpoint
image = pipeline("Astronaut in a jungle, pixelartstyle", image=image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
运行上述代码，初始图像与生成的像素艺术风格的图像的对比如图3 - 45所示。

![image](https://github.com/user-attachments/assets/bc3b5474-63e1-4900-8fef-690488b14bf1)


![图3-45 初始图像与生成的像素艺术风格的图像的对比](此处无实际图片内容，按原文描述应是初始图像与生成的像素艺术风格的图像的对比相关图像)
### 3. 图像到放大器到超分辨率
可以将图像到图像pipeline与放大器和超分辨率pipeline串联，以真正提高图像的细节水平。

首先，从图像到图像pipeline开始。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

image_1 = pipeline(prompt, image=init_image, output_type="latent").images[0]
```
接下来，将该pipeline与放大器pipeline串联，以提高图像分辨率。示例代码如下。
```python
from diffusers import StableDiffusionLatentUpscalePipeline

upscaler = StableDiffusionLatentUpscalePipeline.from_pretrained(
    "stabilityai/sd-x2-latent-upscaler", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
upscaler.enable_model_cpu_offload()
upscaler.enable_xformers_memory_efficient_attention()

image_2 = upscaler(prompt, image=image_1, output_type="latent").images[0]
```
最后，将串联后的pipeline与超分辨率pipeline串联，以进一步提高分辨率。示例代码如下。
```python
from diffusers import StableDiffusionUpscalePipeline

super_res = StableDiffusionUpscalePipeline.from_pretrained(
    "stabilityai/stable-diffusion-x4-upscaler", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
super_res.enable_model_cpu_offload()
super_res.enable_xformers_memory_efficient_attention()

image_3 = super_res(prompt, image=image_2).images[0]
make_image_grid([init_image, image_3.resize((512, 512))], rows=1, cols=2)
```

#### 3.5.5 控制图像生成
尝试生成完全符合你的想象的图像可能很困难，这就是为什么控制生成技术和模型如此有用。虽然你可以使用negative_prompt部分控制图像生成，但还有更健壮的方法，如提示加权和ControlNets。
### 1. 提示加权
关于提示加权的更多信息，请参见3.4.3节。Compel库提供了一个简单的语法来调整 



### 提示加权
读者可以在提示加权指南中了解如何创建嵌入。可以将嵌入传递给AutoPipelineForImage2Image的prompt_embeds（如果使用negative_prompt，则为negative_prompt_embeds）参数，它可以替代prompt参数。示例代码如下。
```python
from diffusers import AutoPipelineForImage2Image
import torch

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image = pipeline(prompt_embeds=prompt_embeds,  # generated from Compel
                 negative_prompt_embeds=negative_prompt_embeds,  # generated from Compel
                 image=init_image,
                 ).images[0]
```
### 2. ControlNet
关于ControlNet的更多信息，请参见3.4.3节。这里通过一个示例来介绍。

首先，用深度图作为条件对图像进行条件处理，以保留图像中的空间信息。示例代码如下。
```python
from diffusers.utils import load_image, make_image_grid

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)
init_image = init_image.resize((958, 960))  # resize to depth image dimensions
depth_image = load_image("https://huggingface.co/lllyasviel/control_v11flip_sd15_depth/resolve/main/images/control.png")
make_image_grid([init_image, depth_image], rows=1, cols=2)
```
初始图像如图3 - 46所示。
![图3-46 初始图像](此处无实际图片内容，按原文描述应是初始图像相关图像)

![image](https://github.com/user-attachments/assets/54b6f195-0526-49b1-879e-162741b74ed8)


然后，加载一个以深度图为条件的ControlNet模型和AutoPipelineForImage2Image。示例代码如下。
```python
from diffusers import ControlNetModel, AutoPipelineForImage2Image
import torch

controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11flip_sd15_depth", torch_dtype=torch.float16, variant="fp16", use_safetensors=True)
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()
```
运行上述代码，生成的图像如图3 - 47所示。
![图3-47 深度图像](此处无实际图片内容，按原文描述应是深度图像相关图像)

![image](https://github.com/user-attachments/assets/0ac48bd7-eb69-47b5-b273-c8adcf283c95)


接下来，根据初始图像（见图3 - 46）、深度图像（见图3 - 47）和提示生成新的图像。示例代码如下。
```python
prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
# prompt = "宇航员在丛林中，冷色调，柔和色彩，细节丰富，8k"
image_control_net = pipeline(prompt, image=init_image, control_image=depth_image).images[0]
make_image_grid([init_image, depth_image, image_control_net], rows=1, cols=3)
```

最后，将新的风格应用到ControlNet生成的图像上，使其与图像到图像pipeline串联。示例代码如下。
```python
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "nitrosocke/elden-ring-diffusion", torch_dtype=torch.float16,
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

prompt = "elden ring style astronaut in a jungle"  # include the token "elden ring style" in the prompt
negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"

image_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image_control_net, strength=0.45, guidance_scale=10.5).images[0]
make_image_grid([init_image, depth_image, image_control_net, image_elden_ring], rows=2, cols=2)
```
运行上述代码，生成的图像如图3 - 48所示。
![图3-48 基于初始图像和深度图像的ControlNet生成效果](此处无实际图片内容，按原文描述应是基于初始图像和深度图像的ControlNet生成效果相关图像)

![image](https://github.com/user-attachments/assets/0bdb6e8b-8fa1-42d1-8e20-97c186c7d749)


### 3.6 图像修复
图像修复（inpainting）用于替换或编辑图像的特定区域，这使其成为一种用于图像修复的实用工具，如去除缺陷和伪影，或者用全新内容替换图像区域。图像修复依赖于遮罩来确定填充图像的哪些区域；需要修复的区域由白色像素表示，保留的区域由黑色像素表示。白色像素由提示填充。

#### 3.6.1 使用Diffusers进行图像修复
使用Diffusers进行图像修复的步骤如下。
**第1步**，使用AutoPipelineForInpainting类加载一个图像修复checkpoint。这会根据checkpoint自动检测要加载的适当pipeline类并应用。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()
```
**第2步**，加载初始图像和遮罩图像。示例代码如下。
```python
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")
```
**第3步**，创建一个提示，并将其连同基础图像和遮罩图像一起传递给pipeline。示例代码如下。
```python
prompt = "a black cat with glowing eyes, cute, adorable, disney, pixar, highly detailed, 8k"
negative_prompt = "bad anatomy, deformed, ugly, disfigured"
image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
每个阶段涉及的图像如图3 - 49~图3 - 51所示。
- 图3 - 49 初始图像
- 图3 - 50 遮罩图像
- 图3 - 51 生成的图像

![image](https://github.com/user-attachments/assets/bab46f65-ec09-4458-94d7-5036e4d4b010)


为方便起见，本书所有示例代码都提供了遮罩图像。读者可以在自己的图像上进行修复，但需要为其创建一幅遮罩图像。可以使用下面的空间轻松创建遮罩图像。上传要进行修复的基础图像，并使用绘图工具绘制遮罩。完成后，单击“Run”按钮以生成并下载遮罩图像，如图3 - 52所示。
![图3-52 绘图工具示例](此处无实际图片内容，按原文描述应是绘图工具示例相关图像)

![image](https://github.com/user-attachments/assets/d462ed34-7434-4395-8ff6-f4428fcc1227)


#### 3.6.2 常用的模型
在流行的修复模型中，Stable Diffusion Inpainting、Stable Diffusion XL Inpainting和Kandinsky 2.2 Inpainting是最受欢迎的。Stable Diffusion XL通常比Stable Diffusion v1.5产生更高分辨率的图像，Kandinsky 2.2也能生成高质量的图像。
### 1. Stable Diffusion Inpainting
Stable Diffusion Inpainting是一个在512像素×512像素图像上针对修复任务进行微调的潜在扩散模型。它是一个很好的起点，因为它相对快速且能生成高质量图像。要使用此模型进行修复，需要将提示、基础图像和遮罩图像传递给pipeline。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

generator = torch.Generator("cuda").manual_seed(92)
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
### 2. Stable Diffusion XL Inpainting
关于Stable Diffusion XL的更多信息，请参见3.4.1节。使用Stable Diffusion XL Inpainting的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "diffusers/stable-diffusion-xl-1.0-inpainting-0.1", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

generator = torch.Generator("cuda").manual_seed(92)
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
### 3. Kandinsky 2.2 Inpainting
关于Kandinsky 2.2的更多信息，请参见3.4.1节。使用Kandinsky 2.2 Inpainting的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

generator = torch.Generator("cuda").manual_seed(92)
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
运行上述代码，生成的图像如图3 - 53~图3 - 56所示。 
