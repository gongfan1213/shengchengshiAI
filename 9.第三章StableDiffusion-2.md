### 3.2.4 Stable Diffusion理论的实际应用（续）
由于这段代码的逻辑与之前提到的txt2img类似，因此这里主要介绍涉及的重点步骤。
- **第1步**：设置文本提示。定义文本提示和负面提示。
- **第2步**：编码文本提示。使用模型的分词器和文本编码器将文本提示转换为嵌入向量。
- **第3步**：初始化图像编码。如果初始图像（init_image）是PIL图像对象，则对其进行预处理并编码成潜在空间向量。
- **第4步**：添加噪声。根据模型的调度器（scheduler）和噪声生成器，向初始潜在向量添加噪声。
- **第5步**：迭代去噪过程。通过循环，模型逐步去除噪声，使用UNet来预测噪声残差，并根据文本嵌入进行分类器自由引导。
- **第6步**：解码图像。将最终的潜在空间向量解码成图像。

示例代码如下：
```python
@torch.no_grad()
def generate_img2img_simplified():
    prompt = ["A fantasy landscape, trending on artstation"]
    negative_prompt = [""]
    strength = 0.5  # strength of the image conditioning
    batch_size = 1
    num_inference_steps = 25
    init_image = init_img

    # set timesteps
    pipe.scheduler.set_timesteps(num_inference_steps)

    # get prompt text embeddings
    text_inputs = pipe.tokenizer(
        prompt,
        padding="max_length",
        max_length=pipe.tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt",
    )
    text_input_ids = text_inputs.input_ids
    text_embeddings = pipe.text_encoder(text_input_ids.to(pipe.device))[0]

    # get unconditional embeddings for classifier free guidance
    uncond_tokens = negative_prompt
    max_length = text_input_ids.shape[-1]
    uncond_input = pipe.tokenizer(
        uncond_tokens,
        padding="max_length",
        max_length=max_length,
        truncation=True,
        return_tensors="pt",
    )
    uncond_embeddings = pipe.text_encoder(uncond_input.input_ids.to(pipe.device))[0]

    # For classifier free guidance, we need to do two forward passes.
    # Here we concatenate the unconditional and text embeddings into a single batch
    # to avoid doing two forward passes
    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])

    # encode the init image into latents and scale the latents
    latents_dtype = text_embeddings.dtype
    if isinstance(init_image, PIL.Image.Image):
        init_image = init_preprocess(init_image)
        init_image = init_image.to(device=pipe.device, dtype=latents_dtype)
        init_latent_dist = pipe.vae.encode(init_image).latent_dist
        init_latents = 0.18215 * init_latent_dist.sample(generator=generator)
        init_latents = init_latents

    # get the original timestep using init_timestep
    offset = pipe.scheduler.config.get("steps_offset", 0)
    init_timestep = int(num_inference_steps * strength) + offset
    init_timestep = min(init_timestep, num_inference_steps)

    timesteps = pipe.scheduler.timesteps[-init_timestep]
    timesteps = torch.tensor([timesteps] * batch_size, device=pipe.device)

    # add noise to latents using the timesteps
    noise = torch.randn(init_latents.shape, generator=generator, device=pipe.device, dtype=latents_dtype)
    init_latents = pipe.scheduler.add_noise(init_latents, noise, timesteps)
    latents = init_latents

    t_start = max(num_inference_steps - init_timestep + offset, 0)
    # Some schedulers like PNDM have timesteps as arrays
    # It's more optimized to move all timesteps to correct device beforehand
    timesteps = pipe.scheduler.timesteps[t_start:].to(pipe.device)

    for i, t in enumerate(pipe.progress_bar(timesteps)):
        # expand the latents if we are doing classifier free guidance
        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents
        latent_model_input = pipe.scheduler.scale_model_input(latent_model_input, t)

        # predict the noise residual
        noise_pred = pipe.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample

        # perform guidance
        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)

        # compute the previous noisy sample x_t -> x_t-1
        latents = pipe.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample

    latents = 1 / 0.18215 * latents
    image = pipe.vae.decode(latents).sample
    image = (image / 2 + 0.5).clamp(0, 1)
    image = image.cpu().permute(0, 2, 3, 1).numpy()
    return image
```

### 3.2.5 Stable Diffusion的内部结构
#### 1. 辅助输出函数
首先，这里会定义一个函数，以便你深入了解扩散模型的内部结构。通过调整deepest参数，你可以根据需要选择查看模型的详细程度。通过设置这个参数，你可以在获取全面概览和深入特定细节之间灵活切换，从而更好地理解模型的工作原理和组成。示例代码如下。
```python
def recursive_print(module, prefix="", depth=0, deepest=3):
    """Simulating print(module) for torch.nn.Modules
    but with depth control. Print to the 'deepest' level. 'deepest=0' means no print
    """
    if depth == 0:
        print(f"[{type(module).__name__}]")
    if depth >= deepest:
        return
    for name, child in module.named_children():
        if len([*child.named_children()]) == 0:
            print(f"{prefix}{name}: {child}")
        else:
            if isinstance(child, nn.ModuleList):
                print(f"{prefix}{name}: {type(child).__name__} len={len(child)}")
            else:
                print(f"{prefix}{name}: {type(child).__name__}")
            recursive_print(child, prefix + "  ", depth + 1, deepest) 
```

### 2. 文本编码模型
首先查看文本编码模型的内容。示例代码如下。
```python
recursive_print(pipe.text_encoder, deepest=3)
```
运行上述代码，输出如下。
```
[CLIPTextModel]
(text_model): CLIPTextTransformer
(embeddings): CLIPTextEmbeddings
  (token_embedding): Embedding(49408, 768)
  (position_embedding): Embedding(77, 768)
(encoder): CLIPEncoder
  (layers): ModuleList len=12
(final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
```
其中比较重要的一个结构是编码器。通过深入观察编码器的结构，可以发现它基本上是由一系列Transformer模块构成的，具体输出如下。
```python
recursive_print(pipe.text_encoder.text_model.encoder, deepest=3)
```
```
[CLIPEncoder]
(layers): ModuleList len=12
(0): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(1): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(2): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(3): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(4): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(5): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(6): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(7): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(8): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(9): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(10): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(11): CLIPEncoderLayer
  (self_attn): CLIPAttention
  (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (mlp): CLIPMLP
  (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
```
每一层又由自注意力模块（self_attn）、多层感知机（mlp）等组成。它们的具体实现如下。
```python
recursive_print(pipe.text_encoder.text_model.encoder.layers[0], deepest=3)
```
运行上述代码，输出如下。
```
[CLIPEncoderLayer]
(self_attn): CLIPAttention
  (k_proj): Linear(in_features=768, out_features=768, bias=True)
  (v_proj): Linear(in_features=768, out_features=768, bias=True)
  (q_proj): Linear(in_features=768, out_features=768, bias=True)
  (out_proj): Linear(in_features=768, out_features=768, bias=True)
(layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
(mlp): CLIPMLP
  (activation_fn): QuickGELUActivation()
  (fc1): Linear(in_features=768, out_features=3072, bias=True)
  (fc2): Linear(in_features=3072, out_features=768, bias=True)
(layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
```
根据列出的结构，结合已有的知识，我们可以分析出每一个模块的具体功能。
- **(self_attn): CLIPAttention**：这是自注意力机制的一个实现，专为CLIP模型定制。它使模块能够专注于输入序列中不同部分的相关性和上下文信息。
- **(k_proj), (v_proj), (q_proj)**：这些是线性层，分别用于生成键（key）、值（value）和查询（query）向量，是自注意力机制的核心部分。这些向量用于计算输入数据中不同部分之间的关联性。 
- **(out_proj)**：另一个线性层，用于将注意力机制的输出转换为下一阶段所需的格式。 
- **(layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)**：这是第1个层规范化（layer normalization）部分，用于规范化自注意力层的输出。层规范化有助于加速训练并提高模型的稳定性。 
- **(mlp): CLIPMLP**：这是一个多层感知机（MLP），用于在自注意力层之后进一步处理数据。它通常包含几个全连接层，对信息进行更深层次的转换和整合。 
- **(activation_fn): QuickGELUActivation()**：这是一个激活函数，用于引入非线性，提高模型的表达能力。 
- **(fc1)和(fc2)**：这两个线性层用于进一步处理和转换数据。第1个线性层将特征维度扩展，第2个线性层再将其缩减，这样的设计有助于增强模型的学习能力。 
- **(layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)**：这是第2个层规范化部分，用于规范化多层感知机的输出，进一步稳定模型的训练过程。

### 3. UNet模型
接下来，我们将深入探究Stable Diffusion中最复杂的部分——UNet。UNet的核心架构包括下采样块（down_blocks）、中间块（mid_block）和上采样块（up_blocks）。 

## qa
### Spatial Transformer（空间变换器）
**1. 功能与动机**
- Spatial Transformer（STN）是一种用于处理图像或特征图空间变换的神经网络模块，旨在解决卷积神经网络（CNN）在处理输入数据时缺乏空间不变性的问题。它能够使模型对平移、缩放、旋转和更通用的扭曲等变换具有不变性。
- 与传统的池化层不同，STN可以通过动态变换整个特征图来提取关键信息，而不是简单地压缩局部信息。

**2. 组成部分**
- **定位网络（Localization Network）**：这是STN的核心部分，负责学习输入图像的空间变换参数。它通常是一个卷积神经网络，输出一个变换矩阵的参数。
- **网格生成器（Grid Generator）**：根据定位网络输出的变换参数，生成一个采样网格，定义了从输入特征图中采样的位置。
- **采样器（Sampler）**：根据网格生成器的输出坐标网格，从输入图像中采样像素，生成变换后的输出图像。

**3. 工作流程**
- STN通过定位网络学习输入图像的变换参数，然后通过网格生成器和采样器对图像进行空间变换，例如旋转、缩放或裁剪。这种变换是可微分的，因此可以通过反向传播进行训练。

**4. 优势**
- STN能够动态调整输入图像，使其更适合后续的特征提取和分类任务。它还可以减少对池化层的依赖，从而提高识别准确性。

### ResNetBlock2D（二维残差块）
**1. 功能与动机**
- ResNetBlock2D是ResNet架构中的一个基本单元，用于构建深度卷积神经网络。它通过引入残差学习机制，解决了深层网络中梯度消失和梯度爆炸的问题。

**2. 结构**
- **输入**：ResNetBlock2D通常有两个输入，一个是来自上一层的特征图（latent），另一个是来自时间步编码模块的输出（time_embeds）。
- **卷积层**：包含两个3×3的卷积层，每个卷积层后通常接一个ReLU激活函数。
- **残差连接**：输入特征图直接加到第二个卷积层的输出上。如果输入和输出的通道数不一致，会通过一个1×1的卷积层进行通道匹配。

**3. 工作原理**
- ResNetBlock2D的核心思想是通过残差学习来简化网络训练。它允许网络学习输入和输出之间的残差（即差异），而不是直接学习映射函数。这种机制使得深层网络能够更容易地训练，并且能够学习到更复杂的特征。

**4. 应用**
- ResNetBlock2D广泛应用于图像分类、目标检测等任务中，是许多深度学习模型的基础组件。它也可以与其他模块（如Transformer模块）结合，用于更复杂的任务，如图像生成。

  ## -end

  
- **下采样块**：负责逐步降低图像的空间分辨率，同时增加特征的深度。在这个过程中，模型能够捕获更高层次的特征和更广泛的上下文信息。
- **中间块**：位于网络的最深层，这个块处理通过下采样块得到的高级特征，执行关键的特征转换和整合。
- **上采样块**：执行相反的过程，逐步增加图像的空间分辨率，同时减少特征的深度。通过这个过程，模型能够重建出细节丰富的图像。

UNet模型UNet2DConditionModel的代码如下。
```python
recursive_print(pipe.unet, deepest=2)
```
运行上述代码，输出如下。
```
[UNet2DConditionModel]
(convy_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(time_proj): Timesteps()
(time_embedding): TimestepEmbedding
  (linear_1): Linear(in_features=320, out_features=1280, bias=True)
  (act): SiLU()
  (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
(down_blocks): ModuleList len=4
  (0): CrossAttnDownBlock2D
  (1): CrossAttnDownBlock2D
  (2): CrossAttnDownBlock2D
  (3): DownBlock2D
(up_blocks): ModuleList len=4
  (0): UpBlock2D
  (1): CrossAttnUpBlock2D
  (2): CrossAttnUpBlock2D
  (3): CrossAttnUpBlock2D
(mid_block): UNetMidBlock2DCrossAttn
  (attentions): ModuleList len=1
  (resnets): ModuleList len=2
(convy_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)
(convy_act): SiLU()
(convy_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
```

#### 1) 上采样层和下采样层分析
首先介绍下采样层。一个CrossAttnDownBlock2D内部基本上是由注意力机制（attentions）和残差网络（resnets）的双层组合构成的，就像两层三明治。在CrossAttnUpBlock2D中，也可以看到类似的结构。

在这种设计中，注意力层和残差网络层的结合使得CrossAttnDownBlock2D和CrossAttnUpBlock2D能够有效地处理和生成图像特征，同时保持结构的稳定性和效率。这种“双层三明治”结构提供了强大的功能来捕获和生成复杂的图像内容。示例代码如下。
```python
recursive_print(pipe.unet.down_blocks[0], deepest=2)
```
运行上述代码，输出如下。
```
[CrossAttnDownBlock2D]
(attentions): ModuleList len=2
  (0): SpatialTransformer
  (1): SpatialTransformer
(resnets): ModuleList len=2
  (0): ResnetBlock2D
  (1): ResnetBlock2D
(downsamplers): ModuleList len=1
  (0): Downsample2D
```
在上采样层中，模块的组成结构差别不大。示例代码如下。
```python
recursive_print(pipe.unet.up_blocks[2], deepest=2)
```
运行上述代码，输出如下。
```python
[CrossAttnUpBlock2D]
(attentions): ModuleList len=3
  (0): SpatialTransformer
  (1): SpatialTransformer
  (2): SpatialTransformer
(resnets): ModuleList len=3
  (0): ResnetBlock2D
  (1): ResnetBlock2D
  (2): ResnetBlock2D
(upsamplers): ModuleList len=1
  (0): Upsample2D
```
可以看到，无论是上采样层还是下采样层，都主要由空间变换器（SpatialTransformer）和残差网络块（ResnetBlock2D）构成。因此，只要理解SpatialTransformer和ResnetBlock2D，基本上就可以理解这个网络的构建块。

#### 2) 空间变换器分析
空间变换器基本上是由以下几部分组成的。
- **自注意力机制**：这一部分使模型能够聚焦于输入特征的不同区域，理解这些区域之间的内在关联。自注意力机制有助于模型捕获图像中的局部细节和上下文信息。
- **交叉注意力（cross - attention）**：这一部分允许模型在两个不同的输入集（例如，文本和图像特征）之间建立联系。交叉注意力在结合模型中多种信息源以生成图像时发挥关键作用。 
- **前馈网络（feed - forward network）**：这是模型中的另一个重要组成部分，用于进一步处理和精炼通过注意力机制得到的特征。

示例代码如下。
```python
recursive_print(pipe.unet.down_blocks[0].attentions[0], deepest=3)
```
运行上述代码，输出如下。
```python
[SpatialTransformer]
(norm): GroupNorm(32, 320, eps=1e-06, affine=True)
(proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
(transformer_blocks): ModuleList len=1
  (0): BasicTransformerBlock
    (attn1): CrossAttention
    (ff): FeedForward
    (attn2): CrossAttention
    (norm1): LayerNorm(320, eps=1e-05, elementwise_affine=True)
    (norm2): LayerNorm(320, eps=1e-05, elementwise_affine=True)
    (norm3): LayerNorm(320, eps=1e-05, elementwise_affine=True)
    (proj_out): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
```
接下来，我们探讨其中的两个注意力层。在关注两个注意力层（attn1和attn2）时，我们可以探讨它们之间的差异。特别是，我们可以猜测哪个张量的形状会有所不同。这些张量包括查询（Q）、键（K）和值（V）。在不同的注意力层中，张量的形状变化通常取决于它们所处理的输入数据类型和尺寸。示例代码如下。
```python
recursive_print(pipe.unet.down_blocks[0].attentions[0].transformer_blocks[0].attn1, deepest=3)
recursive_print(pipe.unet.down_blocks[0].attentions[0].transformer_blocks[0].attn2, deepest=3)
```
一般情况下，Q（查询）通常与正在处理的特定输入数据（例如，特定图像或文本片段）的尺寸有关；K（键）和V（值）的形状通常与模型正在参考或“关注”的数据的尺寸有关。

在attn1和attn2层中，如果它们处理的输入数据或它们所参考的上下文数据在尺寸上有所不同，那么其中一个或多个张量（Q、K、V）的形状可能会有所不同。

例如，如果attn1层专注于处理更细节的图像特征，而attn2层处理的是更高层次的上下文信息，那么这可能导致Q、K、V张量之一在两个层中具有不同的形状。通常情况下，如果输入数据的维度或上下文的复杂性不同，那么Q张量的形状最有可能发生变化。

最后，我们来看看前馈网络。

这里使用了一种特殊的激活函数——GEGLU。在GEGLU中，proj的输出被分成两半，一半用于控制一个Sigmoid门控，另一半用于生成激活。大多数情况下，可以使用更简单的激活函数，如GeLU或SiLU。

GEGLU激活函数的特点如下。
- **双分支设计**：将输出分成两部分，一种方式是用于门控（通常是Sigmoid函数），另一种是用于创建实际的激活信号。这种设计允许网络在决定如何激活神经元时进行更细致的控制。
- **门控机制**：Sigmoid门控允许模型动态地调整不同神经元的激活程度，从而提供更灵活的特征表示。 
- **激活部分**：激活部分负责将处理后的信号转化为模型可以使用的形式，进一步进行特征提取和转换。

它的结构如下。
```python
recursive_print(pipe.unet.down_blocks[0].attentions[0].transformer_blocks[0].ff, deepest=3)
```
运行上述代码，输出如下。
```python
[FeedForward]
(net): Sequential
  (0): GEGLU
    (proj): Linear(in_features=320, out_features=2560, bias=True)
    (1): Dropout(p=0.0, inplace=False)
    (2): Linear(in_features=1280, out_features=320, bias=True)
```

#### 3) ResNet块分析
ResNet块（ResnetBlock2D）是UNet中最简单的部分，基本上就是类似于ResNet的卷积神经网络。ResNet的核心特点是残差连接，它们允许输入直接跳过某些层，从而解决深度网络中的梯度消失问题。它的结构如下。
```python
recursive_print(pipe.unet.down_blocks[0].resnets[0], deepest=3)
```
运行上述代码，输出如下。
```python
[ResnetBlock2D]
(norm1): GroupNorm(32, 320, eps=1e-05, affine=True)
(convy1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)
(norm2): GroupNorm(32, 320, eps=1e-05, affine=True)
(dropout): Dropout(p=0.0, inplace=False)
(convy2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(nonlinearity): SiLU()
```
尽管UNet的ResNet块结构相对简单，但它有效地结合了残差连接的优势和卷积神经网络的特性。它的主要组成模块如下。
- **批量归一化（GroupNorm）**：用于网络层之间的归一化处理，有助于加快训练速度，提高模型稳定性。
- **卷积层（Conv2d）**：负责提取图像中的特征。这些层通过滤波器学习图像的局部模式。
- **激活函数（Activation Function）**：如SiLU，用于引入非线性，增强模型的表达能力。

需要注意的是，time_emb_proj是一个线性投影，它输出用于每个通道的时间调制信号。在稳定扩散模型中，时间调制是一个关键概念，它允许模型根据不同的时间步骤调整其行为，从而在图像生成过程中逐步细化和改进图像特征。

至此，上采样层、下采样层的主要模块的分析就结束了。接下来将进行UNet其他主要模块的介绍。 
  
