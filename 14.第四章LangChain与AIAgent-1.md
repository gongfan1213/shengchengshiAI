### 第4章 LangChain与AI Agent
本章将介绍LangChain与AI Agent。LangChain是目前非常流行的搭建AI Agent的底层框架。AI Agent能通过大语言模型（Large Language Model，LLM）帮助人们完成各类复杂任务。

### 4.1 LangChain与AI Agent简介
#### 1. LangChain简介
LangChain是一个基于语言模型的用来开发应用程序的开源框架。它能让从事人工智能和机器学习的工作者把大语言模型和其他外部组件结合起来，开发出基于大语言模型的应用程序。LangChain的目标是让强大的大语言模型（如OpenAI的GPT - 3.5和GPT - 4）和各种外部数据源连接起来，从而创建并享受NLP应用程序带来的好处。如果用户熟悉Python、JavaScript或TypeScript编程语言，就可以使用LangChain为这些编程语言提供的包。

**为什么LangChain重要呢？主要原因有如下两点**：
- **流程优化**：LangChain是一个简化生成式AI应用接口创建过程的框架。从事此类接口开发的开发者可以使用各种工具来创建优秀的NLP应用程序，而LangChain能够优化这一流程。例如，大语言模型需要访问大量的数据，而LangChain能对这些数据进行组织，使其易于访问。
- **数据更新**：通常GPT模型在发布之前都是基于公开数据进行训练的。例如，ChatGPT是在2022年年底发布的，但其知识库仅限于2021年及之前的数据。LangChain可以将AI模型连接到数据源，使它们能够无限制地获取最新的数据知识。

LangChain通常通过与大语言模型提供商和外部数据源进行集成来构建AI应用程序。AI应用程序通过这些数据源可以方便查找和存储数据。例如，LangChain可以通过将大语言模型（如Hugging Face、Cohere和OpenAI提供的大语言模型）与数据源或存储系统（如Apify Actors、Google Search和Wikipedia）集成来构建聊天机器人或问答系统。这使得AI应用程序能够接收用户输入的文本，对其进行处理，并从这些源中检索最佳答案。在这个意义上，LangChain通过集成利用了最新的NLP技术以构建有效的AI应用程序。图4 - 1展示了LangChain的集成形式。

其他潜在的集成还包括云存储平台，如AWS、Google Cloud和微软Azure，以及向量数据库等。向量数据库可以存储大量高维数据（如视频、图像和长文本等），并将其作为数据表示形式进行存储，从而使AI应用程序更容易查询和检索数据元素。

![image](https://github.com/user-attachments/assets/f269d783-4b66-4414-9209-4228a746d573)


#### 2. AI Agent简介
AI Agent是指一个能够代表用户或其他实体在特定环境中进行交互和决策的软件系统（见图4 - 2）。它具备感知、决策和行动的能力，能够根据环境和目标制定策略并执行任务。而LangChain则是一个框架，用于简化生成式AI应用接口的创建过程，它整合了大语言模型和其他组件，使开发者能够更方便地构建NLP应用程序。

![image](https://github.com/user-attachments/assets/fbd01ebe-3b89-4cc9-8727-ff4ec7abbc62)


**小提示**：想象一下，AI Agent就像一个智能助手，它能帮助我们完成各种任务，比如预订机票、安排日程或者回答问题。而LangChain就像这个助手的“大脑”，它提供了强大的NLP能力，让AI Agent可以理解我们的需求，并以自然语言的形式给出回应。

虽然LangChain本身不直接扮演AI Agent的角色，但它为构建具备智能决策和交互能力的AI Agent提供了重要的工具和组件。通过整合LangChain，AI Agent能够利用大语言模型的能力，实现更高级别的自然语言理解和生成，从而提升与用户或其他实体的交互体验。同时，LangChain的模块化设计也使得AI Agent的开发过程更加灵活和高效。

### 4.1.1 LangChain架构
LangChain是一个用于开发由语言模型驱动的应用程序的框架，旨在帮助开发人员使用语言模型构建端到端的应用程序。它提供了一套工具、组件和接口，可简化创建由大语言模型和聊天模型提供支持的应用程序的过程。LangChain让应用程序能够具备以下特点：
- 具有上下文意识，能够连接到各种上下文源（如提示指令、范例、内容等）。
- 依靠语言模型进行推理，比如基于提供的上下文决定如何回答问题或采取何种行动。

**LangChain架构中的关键部分如下**：
- **Prompts（提示）**：这在大语言模型中是非常重要的，它确保数据以适合大语言模型处理的方式输入，这包括Prompts管理、Prompts优化和Prompts序列化。无论是聊天机器人还是AI绘画，提示都是不可或缺的。
- **Language Models（语言模型）**：这是框架的核心部分，通过通用接口调用大语言模型（例如，GPT - 4等大语言模型）。这使得开发者能够利用大语言模型的能力进行自然语言理解和生成。LangChain对各家公司的大语言模型进行了抽象和封装，提供了通用的API，使开发人员可以方便地使用这些模型。
- **Output Parsers（模型输出解析）**：这部分从大语言模型的输出中提取信息，将其转换为开发者可以理解和使用的格式。
- **Data Connections and Retrieval（数据连接和检索）**：LangChain不仅限于处理文本数据，还能够连接其他数据源，如数据库或API，以检索和整合额外的信息。
- **Chains（链）和Agents（代理或智能体）**：链是一系列对各种组件的调用，它帮助实现复杂的交互和任务执行流程。即Chains模块负责管理大语言模型与其他组件的交互，而Agents模块则定义了如何与大语言模型进行交互，包括如何发送指令和接收响应。


为了帮助理解，你可以想象如下一个简单的链路：

1. 用户输入（如问题或请求）首先通过Prompts模块进行处理，转换为适合大语言模型的输入格式。

2. 这个输入被发送到Language Models模块，由大语言模型进行处理。

3. 大语言模型生成输出，这个输出被Output Parsers模块解析，以提取有用的信息。同时，如果大语言模型需要额外的数据来回答问题或执行任务，数据连接和检索模块会连接到其他数据源，获取必要的信息。

4. 所有这些信息通过Chains模块被整合在一起，然后由Agents模块以自然语言的形式返回给用户。

图4 - 3展示了LangChain如何从用户输入开始，通过大语言模型和其他组件的处理，最终生成用户可以理解的输出。在这个过程中，LangChain确保了数据的顺畅流动和高效处理，从而实现强大的NLP功能。

![image](https://github.com/user-attachments/assets/aa805b89-9949-4524-acd6-f37c64132680)


综上所述，LangChain是一个功能丰富的框架，它提供了库、模板、服务和平台，旨在简化NLP应用程序的开发和部署过程。通过LangChain，开发者可以更加高效地利用大语言模型和其他NLP技术，构建出功能强大的应用程序。

### 4.1.2 使用LangChain构建AI Agent示例
本节将从一个简单的大语言模型链开始，它仅依赖提示模板中的信息来响应。首先，构建一个检索链，该链从单独的数据库中获取数据并将其传递到提示模板中。然后，添加聊天记录，以创建对话检索链。这允许你以聊天方式与此大语言模型进行交互，因为它会记住之前的问题。最后，构建一个AI Agent——它利用大语言模型来确定其是否需要获取数据来回答问题。

**首先，导入LangChain和OpenAI的集成包，并获取API密钥。示例代码如下**：
```bash
pip install langchain-openai
export OPENAI_API_KEY="..."
```
**通过设置环境变量或直接传递密钥来初始化模型。示例代码如下**：
```python
from langchain_openai import ChatOpenAI
llm = ChatOpenAI()
# 或采取以下方法导入模型，同时输入密钥
# llm = ChatOpenAI(OPENAI_API_KEY="...")
```
**接下来，我们可以问一下它什么是LangSmith——由于这是训练数据中不存在的内容，因此AI Agent的响应可能不会很好。示例代码如下**：
```python
llm.invoke("how can langsmith help with testing?")
```
**之后，我们可以使用提示模板来引导AI Agent的响应。提示模板用于将初始用户输入转换为更适合大语言模型的输入。示例代码如下**：
```python
from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are world class technical documentation writer."),
    ("user", "{input}")
])
```
**现在，我们可以将这些组合成一个简单的大语言模型链。示例代码如下**：
```python
chain = prompt | llm
```
**然后，我们可以调用它并问相同的问题。它仍然可能不知道答案，但应该会以更适合技术写作的方式回应。示例代码如下**：
```python
chain.invoke({"input": "how can langsmith help with testing?"})
```
ChatModel的输出（链的输出）是一条消息。然而，使用字符串更加方便。因此，我们可以添加一个简单的输出解析器，将聊天消息转换为字符串。示例代码如下：
```python
from langchain_core.output_parsers import StrOutputParser
output_parser = StrOutputParser()
```
**现在，我们可以将其添加到之前的链中。示例代码如下**：
```python
chain = prompt | llm | output_parser
```
**接下来，我们可以调用它并问相同的问题。现在，答案将是一个字符串（而不是ChatMessage）。示例代码如下**：
```python
chain.invoke({"input": "how can langsmith help with testing?"})
```

### 4.2 LangChain关键组件
4.1节简单介绍了LangChain的概念和作用，本节将带领读者初步认识LangChain各种关键组件的使用方法。我们将在介绍的过程中给出示例代码，让读者更好地认识LangChain，为后续的应用程序开发打下基础。

#### 1. 三大关键组件与各模块的关联
在LangChain的生态系统中，Module I/O、Retrieval和Agent这3个关键组件与LangChain Libraries、LangChain Templates、LangServe和LangSmith模块有着密切的关系。接下来我们将了解它们之间的关系。
- **LangChain Libraries**：LangChain Libraries提供了构建NLP应用程序所需的各种工具和组件，其中就包括Module I/O、Retrieval和Agent。这些库为开发者提供了易于使用和集成的接口，使他们能够轻松地组合这些组件来创建自定义的NLP应用程序。
    - **Module I/O**：作为LangChain Libraries的一部分，它提供了标准化的输入和输出处理机制，使开发者可以轻松地与模型进行交互，并将模型的输出转换为实际应用中所需的格式。
    - **Retrieval**：同样作为库的一部分，Retrieval组件提供了从外部数据源检索信息的功能，增强了模型处理复杂任务的能力。
    - **Agent**：Agent作为整合其他组件的关键部分，也包含在LangChain Libraries中。它提供了与模型、Retrieval和其他组件交互的框架，使开发者可以构建出功能强大的NLP应用程序。
- **LangChain Templates**：LangChain Templates提供了一系列易于部署的参考架构，这些架构已经预先集成了Module I/O、Retrieval和Agent等组件。通过使用这些模板，开发者可以快速启动和部署自己的NLP应用程序，而无须从头开始构建整个系统。这些模板利用LangChain Libraries中的组件，为开发者提供了即插即用的解决方案。
- **LangServe**：LangServe是一个用于将LangChain应用程序部署为REST API的库。当开发者使用LangChain Libraries构建了自己的NLP应用程序后，他们可以利用LangServe将应用程序部署为服务，从而使其能够通过标准的HTTP请求进行访问。在这个过程中，Module I/O、Retrieval和Agent等组件将继续在后台发挥作用，处理来自API请求的数据和生成响应。
- **LangSmith**：LangSmith是一个开发者平台，提供了调试、测试、评估和监控LangChain应用程序的功能。在这个平台上，开发者可以对使用Module I/O、Retrieval和Agent等组件构建的NLP应用程序进行性能优化和调试。LangSmith与LangChain Libraries紧密集成，使开发者可以在统一的环境中管理和监控他们的NLP应用程序。

总之，Module I/O、Retrieval和Agent这3个关键组件是LangChain生态系统中不可或缺的部分。它们与LangChain Libraries、LangChain Templates、LangServe和LangSmith等其他组件和工具协同工作，共同支持开发者构建和部署功能强大的NLP应用程序。

#### 2. 三大关键组件如何协同工作
在构建基于大语言模型的NLP应用程序中，Module I/O、Retrieval和Agent这3个关键组件通过协同工作，实现了信息的有效传递、处理与整合，从而为用户提供了高质量的交互体验。我们举一个简单的例子来帮助读者理解LangChain中的Module I/O、Retrieval和Agent这3个关键组件是如何协同工作的。

假设你正在开发一个基于大语言模型的聊天机器人。这个聊天机器人需要能够回答用户的问题，并且有时候需要从互联网上检索信息来提供更准确的回答。
- **Module I/O**：在这个例子中，Module I/O负责接收用户的输入，并将其传递给大语言模型。例如，用户可能输入了一个问题：“明天的天气怎么样？”Module I/O会将这个问题格式化为大语言模型可以理解的格式，并发送给大语言模型。同样，当大语言模型生成回答时，Module I/O会负责接收这个回答，并将其转换为人类可读的格式，比如将文本转换为语音，以便通过聊天机器人的语音输出功能播放给用户。
- **Retrieval**：在这个例子中，Retrieval组件会在大语言模型需要额外信息来回答问题时发挥作用。例如，如果大语言模型不知道明天的具体天气情况，Retrieval组件可能会被触发，从互联网上检索相关的天气信息。这些信息可能来自天气API或其他在线数据源。Retrieval组件会将这些信息检索回来，并传递给大语言模型，以便大语言模型能够基于这些信息生成更准确的答案。
- **Agent**：在这个例子中，Agent扮演着整合者的角色，负责接收用户的输入，调用Module I/O将输入传递给大语言模型，并在需要时触发Retrieval组件来检索额外信息。Agent还负责接收大语言模型的输出和Retrieval组件检索到的信息，并将这些信息整合在一起，生成最终的回答返回给用户。例如，Agent可能会将大语言模型生成的文本回答和从互联网上检索到的天气信息结合起来，生成一个完整的、包含天气信息的回答返回给用户。

通过这个例子，可以看到Module I/O、Retrieval和Agent这3个组件是如何协同工作的，以及它们在构建基于大语言模型的NLP应用程序中的重要性。这些组件共同支持了聊天机器人的功能，使其能够接收用户输入、处理信息、检索额外数据，并最终生成回答返回给用户。

### 4.2.1 Module I/O
Module I/O负责模型的输入和输出。在LangChain中，模型并不直接与初始数据交互，而是通过Module I/O进行数据的输入和输出。这使得开发者可以将模型与其他组件（如数据连接器、检索器等）的耦合度降低，以便更加独立地进行开发、测试和维护，从而更容易地扩展和修改应用程序。
- **输入**：Module I/O负责接收用户输入，并将其转换为模型可以理解的格式。这可能涉及对输入进行预处理、格式化或模板化。
- **输出**：当模型生成输出时，Module I/O负责接收这些输出，并将其转换为开发者或用户可以理解的形式。这可能涉及对输出进行后处理、解析或将输出转换为特定的格式。

本节将介绍两种不同类型的模型——大语言模型和ChatModel。我们主要将介绍如何使用提示（prompts）和模板（templates）来格式化这些模型的输入，以及如何使用输出分析器来处理输出。
- **模型**：首先，导入LangChain和OpenAI的集成包，并获取API密钥。示例代码如下：
```bash
pip install langchain-openai
export OPENAI_API_KEY="..."
```
然后，可以通过设置环境变量或直接传递密钥来初始化模型。示例代码如下：
```python
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI

llm = OpenAI()
chat_model = ChatOpenAI()
# 或采取以下方法导入模型，同时输入密钥
# llm = ChatOpenAI(OPENAI_API_KEY="...")
```
当调用大语言模型和ChatModel时，可以看到它们之间的区别。示例代码如下：
```python
from langchain.schema import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```
可见，大语言模型会返回一个字符串，而ChatModel会返回一条消息。
- **提示模板**：大多数大语言模型应用程序不会直接将用户输入传递给大语言模型。通常，它们会将用户输入添加到一个较大的文本片段中，称为提示模板，以提供有关正在进行的具体任务的附加上下文。

在前面的示例中，我们传递给模型的文本包含生成公司名称的指令。对于我们的应用程序，如果用户只须提供公司/产品的描述，无须为模型提供指令，那么这样的模型将会有很高的适用度。

提示模板就是将从用户输入到完全格式化提示的所有逻辑捆绑在一起。我们从非常简单的示例开始。例如，生成上述字符串的提示可能如下：
```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```
```python
What is a good name for a company that makes colorful socks?
```
PromptTemplates还可以用于生成消息列表。在这种情况下

![image](https://github.com/user-attachments/assets/caf26e19-dd22-4a7d-a098-9c263c489678)


![image](https://github.com/user-attachments/assets/4b70a27f-7929-426b-a2d1-840d466dda94)


![image](https://github.com/user-attachments/assets/06b98bd8-48f2-494d-a842-b6f9ba78b4f8)





### Python
```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```
### Python
What is a good name for a company that makes colorful socks?

PromptTemplates还可以用于生成消息列表。在这种情况下，提示不仅包含有关内容的信息，还包含每条消息的信息（其角色、在列表中的位置等）。在这里，最常见的情况是一系列Chat Message Templates。每个都包含有关如何格式化该消息的指令——它的角色及其内容。示例代码如下。
### Python
```python
from langchain.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```
### Python
```python
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```
### 3. 输出解析器（Output Parsers）
输出解析器将语言模型的初始输出转换为可在下游使用的格式。输出解析器主要包括如下几种类型。
- 将文本从模型输出转换为结构化信息（如JSON）的解析器。
- 将模型输出转换为字符串的解析器。 
- 将调用返回的额外信息（如OpenAI函数调用）转换为字符串的解析器。

下面使用一个简单的输出解析器用于解析逗号分隔的值列表。示例代码如下。
### Python
```python
from langchain.output_parsers import CommaSeparatedListOutputParser

output_parser = CommaSeparatedListOutputParser()
output_parser.parse("hi, bye")
# >> ['hi', 'bye']
```
### 4. 使用LangChain表达式语言来组合
现在，我们可以将所有这些内容组合成一个链条。这个链条将接收输入变量，将它们传递给提示模板以创建提示，再将提示传递给语言模型，然后通过一个（可选的）输出解析器传递输出。这是一种将模块化逻辑打包起来的便捷方式。示例代码如下。
### Python
```python
template = "Generate a list of 5 {text}.\n\n{format_instructions}"

chat_prompt = ChatPromptTemplate.from_template(template)
chat_prompt = chat_prompt.partial(format_instructions=output_parser.get_format_instructions())
chain = chat_prompt | chat_model | output_parser
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

可以看到，以上的语法由LangChain表达式语言（LangChain Expression Language，LCEL）提供支持。具体内容详见后面的介绍。
#### 4.2.2 Retrieval
Retrieval（检索）组件在LangChain中扮演了重要角色。由于大语言模型通常受到上下文长度的限制，它们可能无法在一次交互中处理大量的信息。因此，Retrieval组件被用来在需要时从外部数据源中检索相关信息，以支持模型的后续处理。实现这一目标的主要方法是使用检索增强生成（Retrieval Augmented Generation，RAG）。在这个过程中，会检索外部数据，然后在生成步骤中将这些数据传递给大语言模型。
- **文档检索**：Retrieval组件可以根据用户查询或模型需求，从外部文档集中检索相关信息。这可能涉及使用传统的信息检索技术（如TF-IDF、BM25等）或更先进的技术（如基于向量的检索）。 
- **上下文管理**：除了基本的文档检索功能以外，Retrieval组件还可以用来管理对话或任务的上下文。通过存储和检索先前的对话或任务信息，它可以帮助模型更好地理解当前输入，并生成更准确的响应。

文档加载器（Data Loader）可以从许多不同的来源加载文档。LangChain提供了超过100种不同的文档加载器，以及与空间中的其他主要提供商（如AirByte和Unstructured）的集成。LangChain提供了加载所有类型文档（HTML、PDF、代码）从所有类型位置（私有S3存储桶、公共网站）的集成。

最简单的加载器将文件作为文本读取，并将其全部放入一个文档中。示例代码如下。
### Python
```python
from langchain_community.document_loaders import TextLoader

loader = TextLoader("./index.md")
loader.load()
```
### Python
```python
[
    Document(page_content='---\nsidebar_position: 0\n---\n# Document loaders\n\nUse document loaders to load data from a source as `Document`'s. A `Document` is a piece of text\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\ncontents of any web page, or even for loading a transcript of a YouTube video.\n\nEvery document loader exposes two methods: \n1. "Load": load documents from the configured source\n2. "Load and split": load documents from the configured source and split them using the passed in text splitter\n\nThey optionally implement: \n3. "Lazy load": load documents into memory lazily\n\nmetadata={'source': '../docs/docs/modules/data_connection/document_loaders/index.md'})
]
```
#### 4.2.3 Agent
Agent是LangChain中的另一个核心组件，它负责将各个组件整合在一起，形成一个完整的NLP应用程序。Agent负责接收用户输入、与模型进行交互、调用其他组件（如Retrieval）并生成最终响应。
- **输入处理**：Agent首先接收用户的初始输入，并可能使用Module I/O对其进行预处理或格式化。 
- **与模型交互**：Agent将处理后的输入传递给模型，并接收模型的输出。它可能需要根据模型的输出生成后续输入，以支持多轮对话或任务完成。 
- **调用其他组件**：如果需要，Agent可以调用其他组件（如Retrieval）来获取额外的信息或执行特定的任务。 
- **生成最终响应**：Agent将模型的输出和其他组件的结果整合在一起，生成最终的响应并返回给用户。
### 4.3 LCEL入门
本节将介绍LangChain中最重要的内容——LangChain表达式语言（LCEL）。
#### 1. LCEL是什么
LCEL是LangChain官方推出的一种新的语法，用于通过组合方式创建链。它可以将一些有趣的Python概念抽象成一种格式，从而构建LangChain组件链的“极简主义”代码层。

LCEL支持超快速开发链、流式处理、异步、并行执行等高级特性，并且可以与LangSmith和LangServe等工具集成。此外，LCEL与LangChain库和模板的紧密集成，使开发者能够轻 


松地构建和部署功能强大的NLP应用程序。

通过使用LCEL，开发者可以使用最基本的组件来构建复杂的链，并且支持一些开箱即用的功能，包括流式输出、并行处理以及日志输出等。这使开发者能够更加高效、灵活地构建和部署NLP应用程序，提高开发效率和应用程序性能。

总的来说，LCEL是LangChain生态系统中不可或缺的一部分，它为开发者提供了一种强大的工具，使他们能够更加方便地构建和部署基于大语言模型的NLP应用程序。
### 2. LCEL的优势和特点
LCEL作为LangChain生态系统中的一个重要组件，具有一些显著的优势和特点，使得它在构建和部署NLP应用程序时非常好用。
- LCEL提供了一种声明性的方式来组合和创建链，这使开发者能够以一种更加直观和简洁的方式表达他们的意图。通过简单的语法和组件组合，开发者可以快速地构建出复杂的NLP应用程序，而无须深入了解底层的实现细节。 
- LCEL支持流式处理、异步和并行执行等高级特性，这使开发者能够处理大量的数据和请求，并且能够提高应用程序的性能和响应速度。这对于许多实时性和高并发性的NLP应用程序来说是非常重要的。 
- LCEL与LangChain库、模板以及其他工具（如LangSmith和LangServe）的紧密集成，使开发者能够更加方便地利用这些工具和资源来构建和部署应用程序。这种集成不仅简化了开发过程，还提高了应用程序的可维护性和可扩展性。 
- LCEL还具有一些开箱即用的功能，如流式输出、并行处理以及日志输出等，这些功能使开发者能够更加方便地监控和管理应用程序，以及进行调试和优化。

LCEL作为一种强大而灵活的NLP应用开发工具，具有许多优势和特点，这使其在构建和部署基于大语言模型的NLP应用程序时非常好用。无论是对于初学者还是经验丰富的开发者来说，LCEL都是一个值得考虑的选择。

接下来，通过一个示例帮助读者更好地理解LCEL的特点。假设你正在构建一个聊天机器人应用程序，你希望它能够理解用户的意图，并根据意图生成相应的回答。为了实现这个功能，你可以使用LCEL构建一个简单的链，该链由两个主要组件组成：意图识别模型和响应生成模型。

借助LCEL，你可以通过简单的语法和组件组合来创建这个链。下面是LCEL表达式的一个示例。
### Python
```python
intent_model = load_model("intent_recognition_model")
response_model = load_model("response_generation_model")

chain = [
    {"input": "user_text", "output": "intent"},
    {
        "input": "intent",
        "model": intent_model,
        "output": "predicted_intent"
    },
    {
        "input": "predicted_intent",
        "model": response_model,
        "output": "generated_response"
    }
]
```
在上面的示例中，首先加载了两个模型：一个用于意图识别，另一个用于响应生成。然后，定义了一个链，其中包含3个步骤：第1步将用户的文本输入作为链的输入，并将其命名为“intent”；第2步使用意图识别模型对输入文本进行意图识别，并将结果命名为“predicted_intent”；第3步使用响应生成模型根据预测的意图生成相应的回答，并将结果命名为“generated_response”。

通过LCEL的这种声明性方式，我们可以轻松地组合和创建链，而无须深入了解底层的实现细节。此外，LCEL还支持流式处理、异步和并行执行等高级特性，这意味着我们可以处理大量的数据和请求，并且能够提高应用程序的性能和响应速度。这个例子展示了LCEL的一些特点，包括声明性组合、简单语法和高级特性支持。通过使用LCEL，开发者可以更加高效、灵活地构建和部署NLP应用程序，提高开发效率和应用程序性能。
### 3. 用LCEL创建链的优势
LCEL创建的链在NLP应用程序中具有多种用途。它主要用来处理和生成文本数据，以满足特定的任务需求。LCEL创建的链的主要用途如下。
- **文本处理**：可以对输入的文本进行各种处理，如分词、命名实体识别、情感分析等。这些处理步骤有助于提取文本中的关键信息，为后续的文本生成或理解提供基础。 
- **意图识别**：在对话系统或聊天机器人中，LCEL创建的链可以识别用户的意图，即用户想要做什么或获取什么信息。这有助于系统生成更加准确的回应。 
- **文本生成**：基于输入的文本或识别出的意图，LCEL创建的链可以生成相应的回应或文本。这可以用于生成聊天机器人的回复、自动完成句子、生成摘要等。 
- **信息检索**：在需要查找或获取外部信息的情况下，LCEL创建的链可以触发检索组件，从数据库、互联网或其他资源中检索相关信息，并将其整合到处理流程中。 
- **异步和并行处理**：LCEL创建的链支持异步和并行执行，这意味着它可以同时处理多个任务或请求，提高处理速度和效率。 
- **模块化和可定制性**：LCEL的模块化架构允许轻松定制和修改链组件。这意味着开发者可以根据具体需求添加、删除或修改链中的组件，以实现个性化的功能。 

总的来说，LCEL创建的链是一个强大的工具，用于构建高效、灵活和可扩展的NLP应用程序。通过组合不同的组件和处理步骤，它可以满足各种文本处理任务的需求，提高应用程序的性能和用户体验。这使得其具体的应用场景也非常广泛，主要涉及NLP领域的各种任务和应用。以下是一些具体的应用场景示例。
- **聊天机器人**：LCEL可以用于构建聊天机器人的核心处理流程。通过组合不同的NLP组件，如意图识别、对话管理、文本生成等，聊天机器人可以理解和生成自然语言对话，提供智能问答、闲聊、任务执行等服务。 
- **智能助手**：类似于聊天机器人，智能助手也可以利用LCEL来构建。它们可以帮助用户管理日程、提供生活建议、执行命令等。通过组合不同的NLP组件和第三方服务，智能助手可以实现更加个性化和智能化的功能。 
- **文本摘要和生成**：LCEL可以用于文本摘要和生成任务。它可以结合文本理解、信息抽取和生成模型等组件，自动生成文章的摘要、新闻报道、广告文案等。这对于信息获取、内容创作和广告营销等领域非常有用。 
- **情感分析和情感生成**：通过组合情感分析模型和文本生成模型，LCEL可以用于情感分析和情感生成任务。它可以分析文本中的情感倾向和情感表达，并生成带有特定情感的文本，如情感回复、情感标签等。 
- **问答系统和知识库**：LCEL可以用于构建问答系统和知识库。通过整合知识库、信息检索和问答匹配等组件，问答系统可以回答用户的问题并提供相关的知识。这对于智能客服、在线教育、智能助手等领域非常有用。 
- **文本翻译和本地化**：LCEL可以用于文本翻译和本地化任务。通过结合机器翻译模型、语言模型和文本处理组件，LCEL可以实现多语言之间的文本翻译和本地化处理，帮助用户理解和适应不同语言环境的文本内容。 

这些只是LCEL的一些应用场景示例。实际上，由于LCEL的模块化和可定制性，它可以应用于任何需要NLP处理的场景，如社交媒体分析、舆情监控、智能写作等。开发者可以根据具体需求组合和定制LCEL组件，以满足不同任务和应用的需求。
### 4.4 LCEL的使用示例
#### 4.4.1 基础原理
LCEL的核心思想是：一切皆为对象，一切皆为链。LCEL中的每个对象都实现了一个统一的接口，如Runnable，这定义了一系列的调用方法（如invoke、batch、stream等）。这样，你可以用相同的方式调用不同类型的对象，无论它们是模型、函数、数据还是配置。利用这些接口，用户可以很方便地构建LangChain链。本节将为读者介绍一些常用的LCEL的接口，以及异步和并行处理等内容，并给出它们的使用样例来辅助理解。

以下是一个使用LCEL的简单示例。

首先，导入必要的库和模块，如ChatPromptTemplate、ChatOpenAI和StrOutputParser；然后，创建一个链，将Prompt、大语言模型（如ChatOpenAI）和OpenAI和StrOutputParser连接在一起；最后，这个链可以接收输入，如{"topic":"bears"}，并生成相应的输出。

在LCEL中，可以使用“|”符号来连接不同的组件，形成一个链式结构。示例代码如下。
### Python
```python
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import StrOutputParser

prompt = ChatPromptTemplate.from_template("tell me a short joke about {topic}")
model = ChatOpenAI()
output_parser = StrOutputParser()

# 创建链
chain = prompt | model | output_parser

# 调用链
result = chain.invoke({"topic": "ice cream"})
print(result)
```
在这个示例中，首先，创建一个基于模板的prompt对象；然后，将其与ChatOpenAI模型和一个字符串输出解析器连接在一起；最后，通过调用链的invoke方法来生成关于“ice cream”的笑话。
### 注意
以上示例中的具体库和模块可能因使用的LangChain版本而有所不同。因此，在实际使用时，请确保查阅相关文档以获取最新的信息和示例代码。

此外，LCEL还支持更高级的特性，如流式处理、异步执行和并行化。例如，可以使用chain.stream方法来流式处理输入数据，或使用chain.invoke方法来异步调用链。这些特性可以帮助你提高AI应用程序的性能和响应速度。接下来我们将给出这几种特性的示例，以帮助你更快上手。
#### 1. 流式处理
流式处理允许你处理大量的数据，而不需要一次性将所有数据加载到内存中。这对于处理大文件、数据流或实时数据非常有用，特别是当数据不适合一次性加载到内存中时，例如，处理日志文件、实时数据流、视频帧等。示例代码如下。
### Python
```python
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import StrOutputParser
from langchain.chains import LChain

# 创建组件
prompt = ChatPromptTemplate.from_template("tell me a short joke about {topic}")
model = ChatOpenAI()
output_parser = StrOutputParser()

# 创建链
chain = LChain([prompt, model, output_parser])

# 流式处理数据
def process_data(data_stream):
    for data in data_stream:
        result = chain.stream(data)
        print(result)

# 假设我们有一个生成器，它产生一系列的输入数据
data_generator = ({"topic": f"topic_{i}"} for i in range(10))

# 使用流式处理
process_data(data_generator)
```
流式处理允许你处理一个数据流，而不是一次性加载整个数据集。在示例中，我们创建了一个数据流生成器data_generator，它产生一系列的输入数据。然后，我们使用chain.stream方法逐个处理这些数据，而不是一次性处理所有数据。

流式处理的具体应用领域和示例如下。
- **实时数据分析**：在大数据环境中，流式处理用于实时分析从各种传感器、日志文件、社交媒体等来源接收的数据流。例如，金融交易系统可能需要实时分析股市数据以做出决策。 
- **网络监控**：网络安全团队可以使用流式处理来实时监控网络流量，检测异常行为或潜在的安全威胁。 
- **物联网（Internet of Things，IoT）**：在IoT应用中，流式处理用于从设备传感器收集数据，并在数据到达时进行实时分析和响应。 
#### 2. 异步执行
异步执行允许你在等待某个操作完成时，同时执行其他操作。异步执行通常用于提高应用程序的响应性和吞吐量。例如，在处理网络请求、数据库查询或任何I/O密集型操作时，你可以使用异步编程来避免阻塞主线程，从而提高应用程序的性能。示例代码如下。
### Python
```python
from langchain.chains import AChain
import asyncio

# 创建一个异步版本的链
async_chain = AChain([prompt, model, output_parser])

async def async_process_data(data):
    result = await async_chain.ainvoke(data)
    print(result)

# 使用异步函数处理数据
loop = asyncio.get_event_loop()
loop.run_until_complete(async_process_data({"topic": "async_joke"}))
```
异步执行允许你在等待一个操作完成时执行其他操作。在示例中，我们使用了AChain（假设它是一个异步链类）和async/await语法来异步处理数据。这意味着在等待模型生成回复时，程序可以继续执行其他任务。

异步执行的具体应用领域和示例如下。
- **Web开发**：在Web应用程序中，异步执行通常用于处理用户请求，而不会阻塞主线程。例如，使用异步编程可以实现在等待数据库查询结果时，仍然响应用户的其他操作。 
- **图形用户界面（GUI）**：在GUI应用程序中，异步编程可以提高响应性，使用户在单击按钮或进行其他交互时不会感到延迟。 
- **API服务**：在构建RESTful API或微服务时，异步执行允许服务在处理长时间运行的任务时，仍然能够响应其他请求。 
#### 3. 并行化
并行化允许你同时执行多个任务，从而提高处理速度。这在处理多个独立的任务或数据集时非常有用。例如，当你需要同时处理多幅图像、多个视频帧，或执行复杂的数学计算时，并行化可以显著提高处理速度。示例代码如下。
### Python
```python
from langchain.chains import PChain
from concurrent.futures import ThreadPoolExecutor

# 创建一个并行版本的链
parallel_chain = PChain([prompt, model, output_parser])

def parallel_process_data(data_list):
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(parallel_chain.invoke, data) for data in data_list]
        results = [f.result() for f in futures]
        for result in results:
            print(result)

# 假设我们想要并行处理一个数据列表
data_list = ({"topic": f"parallel_joke_{i}"} for i in range(5))

# 使用并行处理
parallel_process_data(data_list)
```
并行化允许你同时执行多个任务，从而提高处理速度。在示例中，我们使用PChain（假设它是一个并行链类）和ThreadPoolExecutor来并行处理多个输入数据。这意味着多个任务可以同时进行，而不是一个接一个地执行。

并行化的具体应用领域和示例如下。
- **科学计算**：在科学计算领域，如物理模拟、气候模型或生物信息学，并行化可以显著加速计算过程。 
- **机器学习**：在机器学习和深度学习中，训练模型通常需要大量的计算资源。通过并行化，可以在多个处理器或GPU上同时处理不同的训练任务，从而加速训练过程。 
- **图像和视频处理**：在处理大量图像或视频帧时，并行化可以加快处理速度，例如同时进行多个滤镜效果或特征提取。 

总的来说，这些技术可以应用于任何需要处理大量数据、提高性能或响应性的场景。选择使用哪种技术取决于具体的需求和约束条件，如数据类型、处理速度、计算资源等。 


 

#### 4.4.2 常用方式
在LCEL中，有很多基本的使用方式，本小节将介绍一些LCEL的常用用法，包括操作数据、传递数据、自定义函数和创建可运行对象等。在每种用法的介绍中，会给出相应的实现代码，以帮助读者快速掌握基础的使用。
### 1. 操作数据
在LCEL中，可以使用各种操作符和操作函数来操作数据。例如，你可以使用算术操作符（如+、-、*、/）进行数值计算，使用字符串操作符（如+、substring）处理字符串数据。示例代码如下。
### Python
```python
from langchain.chains import LChain
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.output_parsers import StrOutputParser

# 创建一个简单的链，其中包括prompt、model和output_parser
prompt = ChatPromptTemplate.from_template("What is {math_expr}?")
model = ChatOpenAI()
output_parser = StrOutputParser()

# 创建链
chain = LChain([prompt, model, output_parser])

# 传递数据到链中
data = {"math_expr": "2 + 2"}
result = chain.run(data)

# 输出结果
print(result)  # 输出："What is 2 + 2? The answer is 4."
```
### 2. 传递数据
在LCEL中，数据通过链的组件进行传递。每个组件都可以接收输入数据并进行处理，同时产生输出数据。该输出数据随后传递给链中的下一个组件。示例代码如下。
### Python
```python
# 假设我们有一个自定义的输出解析器，它接收模型的输出并返回处理后的数据
class CustomOutputParser:
    def process(self, output):
        # 这里可以添加任何自定义的处理逻辑
        return f"Processed output: {output}"

# 创建一个包含自定义输出解析器的链
custom_parser = CustomOutputParser()
chain = LChain([prompt, model, custom_parser])

# 传递数据到链中
data = {"math_expr": "3 * 3"}
processed_result = chain.run(data)

# 输出处理后的结果
print(processed_result)  # 输出："Processed output: The answer is 9."
```
### 3. 自定义函数
LCEL允许你定义自定义函数，这些函数可以在链的组件中调用。自定义函数可以执行任何你需要的逻辑，从简单的数据处理到复杂的业务规则。示例代码如下。
### Python
```python
# 定义一个自定义函数，用于处理数学表达式
def evaluate_math_expr(expr):
    try:
        return eval(expr)
    except Exception as e:
        return f"Error evaluating expression: {e}"

# 将自定义函数作为链的一部分
custom_prompt = ChatPromptTemplate.from_template("Evaluate the expression {math_expr} and return the result.")
custom_model = lambda data: {"result": evaluate_math_expr(data["math_expr"])}

# 创建链
chain = LChain([custom_prompt, custom_model, output_parser])

# 传递数据到链中
data = {"math_expr": "5 - 2"}
evaluated_result = chain.run(data)

# 输出评估后的结果
print(evaluated_result)  # 输出："Evaluate the expression 5 - 2 and return the result. The answer is 3."
```
### 4. 创建可运行对象
在LCEL中，你可以通过组合不同的组件和函数来创建可运行的对象（通常是链），这些对象可以被配置、保存和重用。示例代码如下。
### Python
```python
# 创建可运行对象（链）
runnable_chain = LChain([prompt, model, output_parser])

# 保存可运行对象（这里仅为示例，实际保存方法取决于LCEL的实现）
save_chain(runnable_chain, "my_chain.lcel")

# 加载可运行对象（这里仅为示例，实际加载方法取决于LCEL的实现）
loaded_chain = load_chain("my_chain.lcel")

# 使用可运行对象处理数据
data = {"math_expr": "4 / 2"}
result = runnable_chain.run(data)

# 输出结果
print(result)  # 输出："What is 4 / 2? The answer is 2."
```
### 注意
以上示例代码假设LChain、ChatPromptTemplate、ChatOpenAI、StrOutputParser等类和函数是LCEL框架中实际存在的。在实际使用时，需要查阅LCEL的官方文档或API查找正确的类和方法。
### 4.5 RAG基础应用
LangChain设计了多个组件，旨在帮助构建问答应用程序，更广泛地支持RAG应用程序。为了熟悉这些组件，我们将构建一个简单的基于文本数据源的问答应用程序。在此过程中，我们将讨论典型的问答架构，探讨相关的LangChain组件，并突出展示更高级问答技术的额外资源。我们还将看到LangSmith如何帮助我们追踪和理解我们的应用程序。随着应用程序复杂性的增加，LangSmith将变得越来越有用。

RAG是一种用于提升大语言模型的知识范围，通过引入额外的数据来增强其能力的技术。尽管大语言模型能够推理各种各样的主题，但它们的知识是基于它们训练时所接触到的公共数据。这就意味着，如果使用者想要构建的AI应用程序需要考虑私人数据，或者需要处理模型截止日期之后引入的新数据，就需要使用RAG技术。

具体而言，RAG技术将特定信息引入模型中，以便模型在生成文本或回答问题时能够考虑这些信息。例如，如果使用者正在构建一个关于医学的AI应用程序，他可能希望模型能够考虑到最新的医学研究结果，而这些结果可能是在模型训练之后才出现的。通过使用RAG，使用者可以将这些最新的研究数据引入模型中，以便模型在生成文本时能够基于这些数据进行推理和决策。
#### 4.5.1 RAG架构
RAG架构如图4 - 4所示。它主要包含如下两个组件。
- **索引**：一个从源数据中摄取数据并对其进行索引的流程，这通常是离线发生的。 
- **检索和生成**：实际的RAG链，它在运行时接受用户查询，并从索引中检索相关数据，然后传递给模型。 



![image](https://github.com/user-attachments/assets/47faf544-a832-45e6-aa56-80111d69f621)



### 1. 索引
索引组件中包括加载、分割和存储3部分。
- **加载**：我们将使用DocumentLoaders来加载我们的数据。 
- **分割**：文本分割器将大型文档分割成更小的块。这对于索引数据和将其传递给模型都很有用，因为大块数据更难搜索，并且不适合模型有限的上下文窗口。 
- **存储**：我们需要一个地方来存储和索引我们的分割，以便它们可以被搜索。这通常使用VectorStore和Embeddings模型来完成。 
### 2. 检索和生成
检索和生成组件包括检索和生成两部分。
- **检索**：给定一个用户输入，相关的分割从存储中通过检索器检索出来。 
- **生成**：一个大语言模型（如ChatModel）使用包含问题和检索到的数据的提示来生成答案。 
#### 4.5.2 设置
### 1. 依赖项
我们将在这个演示中使用OpenAI聊天模型和嵌入以及Chroma向量存储，但这里展示的一切都适用于任何大语言模型（如ChatModel）、Embeddings和VectorStore或检索器。

首先，安装所需的包。示例代码如下。
### Shell
```bash
pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai chromadb bs4
```
然后，设置环境变量OPENAI_API_KEY。可以直接设置或从.env文件中加载。示例代码如下。
### Python
```python
import getpass
import os
os.environ["OPENAI_API_KEY"] = getpass.getpass()
import dotenv
dotenv.load_dotenv()
```
### 2. LangSmith
使用LangChain构建应用程序时包含多个步骤，并多次调用大语言模型。随着这些应用程序变得越来越复杂，能够检查链或Agent内部确切发生的情况变得至关重要。推荐的做法是使用LangSmith。

请注意，LangSmith不是必需的，但它是有帮助的。如果你想使用LangSmith，在你通过上面的链接注册后，确保设置你的环境变量以开始记录跟踪。示例代码如下。
### Python
```python
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```
### 3. 预览
接下来，将构建一个QA（Question Answer，问答）应用程序，该应用程序基于Lilian Weng的“LLM Powered Autonomous Agents”博文询问有关该文章内容的问题。

首先，创建一个简单的索引pipeline和RAG链。示例代码如下。
### Python
```python
import bs4
from langchain import hub
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
```
然后，加载、分块和索引博文的内容。示例代码如下。
### Python
```python
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/,"),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    )
)
docs = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
```
接下来，使用博文的相关片段检索和生成相关内容。示例代码如下。
### Python
```python
retriever = vectorstore.as_retriever()
prompt = hub.pull("rlm/rag-prompt")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)
rag_chain.invoke("什么是任务分解?")
```
# 任务分解是一种将复杂任务分解为更小、更简单步骤的技术。它可以通过提示技术如思维链或思维导图，或者使用任务特定的指令或人类输入来完成。任务分解帮助Agent提前规划，并更有效地管理复杂任务。'

最后，进行清理。示例代码如下。
### Python
```python
vectorstore.delete_collection()
```
#### 4.5.3 详细分析
本节将逐步拆解4.5.2节中的代码，真正理解其中的逻辑。
### 1. 索引：加载
首先加载博文内容。我们可以使用DocumentLoaders完成这一点，这将从源加载数据并返回文档列表的对象。一个Document是一个包含一些page_content(str)和metadata(dict)的对象。

在这种情况下，可以使用WebBaseLoader——它使用urllib从Web URL加载HTML并使用BeautifulSoup将其解析为文本。我们可以通过向BeautifulSoup解析器传递参数via bs_kwargs来自定义HTML到文本的解析。在这种情况下，由于只有带有类“post-content”“post-title”或“post-header”的HTML标签是相关的，因此我们将删除所有其他标签。示例代码如下。
### Python
```python
import bs4
from langchain_community.document_loaders import WebBaseLoader

# 仅保留完整HTML中的文章标题、标头和内容
bs4_strainer = bs4.SoupStrainer(class_=("post-title", "post-header", "post-content"))
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/,"),
    bs_kwargs={"parse_only": bs4_strainer},
)
docs = loader.load()

len(docs[0].page_content)
# 42824
print(docs[0].page_content[:500])

# LLM Powered Autonomous Agents
# Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng
# Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.
# Agent System Overview
# In
```
### 2. 索引：分割
我们加载的文档包含的字符超过42 000个。这对许多模型的上下文窗口来说太长了。即使对那些可以将完整博文放入其上下文窗口的模型来说，在非常长的输入中也难以找到信息。

为了处理这个问题，我们将把文档分割成块，以便进行嵌入和向量存储。这应该有助于我们在运行时只检索博文的最相关部分。

在这种情况下，我们将把文档分割成每个块1000个字符，块之间有200个字符的重叠。重叠有助于减轻语句与相关的重要上下文分开的可能性。我们使用RecursiveCharacterTextSplitter——它会使用常见的分隔符（如换行符）递归地分割文档，直到每个块达到适当的大小。这是通用文本用例推荐的文本分割器。

我们设置add_start_index=True，这样每个分割文档在初始文档中开始的字符索引将被保存为元数据属性“start_index”。示例代码如下。
### Python
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000, chunk_overlap=200, add_start_index=True
)
all_splits = text_splitter.split_documents(docs)
len(all_splits)
# 66
len(all_splits[0].page_content)
# 969
all_splits[10].metadata
# {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',
#  'start_index': 7056}
```
### 3. 索引：存储
现在，我们需要对66个文本块进行索引，以便在运行时可以对它们进行搜索。最常见的方法是将每个文档分割的内容嵌入，并将这些嵌入插入到一个向量数据库（或向量存储）中。当想要搜索已保存的分割时，可以进行文本搜索查询，将其嵌入，并执行某种“相似性”搜索，以识别与我们查询嵌入最相似的存储分割。最简单的相似性度量是余弦相似度——测量每对嵌入之间的夹角的余弦（它们是高维向量）。

我们可以使用Chroma和OpenAIEmbeddings模型在单个命令中嵌入和存储所有的文档分割。示例代码如下。
### Python
```python
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings

vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())
```
### 4. 检索和生成：检索
现在让我们编写实际的应用程序逻辑。我们希望创建一个简单的应用程序，接受用户的问题，搜索与该问题相关的文档，将检索到的文档和初始问题传递给模型，然后返回一个答案。

首先，需要定义在文档上搜索的逻辑。LangChain定义了一个Retriever接口，它包装了一个索引，可以根据字符串查询返回相关的文档。

最常见的检索器类型是VectorStoreRetriever，它使用向量存储的相似性搜索功能进行检索。任何VectorStore都可以轻松地转换为Retriever，这里使用VectorStore.as_retriever方法即可。示例代码如下。
### Python
```python
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})
# search_kwargs参数指的是对查询操作的配置，{"k": 6}指的是返回查询匹配前6位的结果

retrieved_docs = retriever.invoke("What are the approaches to Task Decomposition?")
len(retrieved_docs)
# 6
print(retrieved_docs[0].page_content)

# Tree of Thoughts (Yao et al., 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.
# Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outline." for writing a novel, or (3) with human inputs.
``` 

