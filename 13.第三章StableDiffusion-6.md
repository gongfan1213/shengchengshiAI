![image](https://github.com/user-attachments/assets/07c67ec9-8096-47d4-a58e-8f9e486dfb0f)



### 3.6.3 非特定修复的checkpoint
截至目前，本书使用了特定于修复的checkpoint，如runwayml/stable-diffusion-inpainting。你也可以使用常规checkpoint，如runwayml/stable-diffusion-v1-5。接下来比较一下这两个checkpoint的结果。

使用runwayml/stable-diffusion-v1-5模型时的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16"
).to("cuda")
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

generator = torch.Generator("cuda").manual_seed(92)
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
使用runwayml/stable-diffusion-inpainting模型时的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
).to("cuda")
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

generator = torch.Generator("cuda").manual_seed(92)
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, generator=generator).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
运行上述代码，两个模型生成的图像如图3 - 57和图3 - 58所示。
- 图3 - 57 runwayml/stable-diffusion-v1-5模型生成的图像
- 图3 - 58 runwayml/stable-diffusion-inpainting模型生成的图像

![image](https://github.com/user-attachments/assets/678fc2a6-1ddc-4d2e-b053-5a472cbe75a2)


然而，对于更基础的任务，如从图像中擦除一个对象（例如路上的石头），常规checkpoint的结果相当不错。常规checkpoint和修复checkpoint之间的区别不太明显。

使用runwayml/stable-diffusion-v1-5模型时的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16"
).to("cuda")
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png")

image = pipeline(prompt="road", image=init_image, mask_image=mask_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
使用runwayml/stable-diffusion-inpainting模型时的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
).to("cuda")
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/road-mask.png")

image = pipeline(prompt="road", image=init_image, mask_image=mask_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
运行上述代码，两个模型生成的图像如图3 - 59和图3 - 60所示。
- 图3 - 59 runwayml/stable-diffusion-v1-5模型生成的图像
- 图3 - 60 runwayml/stable-diffusion-inpainting模型生成的图像

![image](https://github.com/user-attachments/assets/4fe19d20-9705-44b0-9f32-ba62478c867d)


使用非特定于修复的checkpoint的权衡因素是整体图像质量可能较低，但非特定于修复的checkpoint通常倾向于保留遮罩区域（这就是为什么你可以看到遮罩轮廓）。特定于修复的checkpoint经过有意训练，以生成更高质量的修复图像，这包括在遮罩和未遮罩区域之间创建更自然的过渡。因此，这些checkpoint更有可能改变你的未遮罩区域。

如果保留未遮罩区域对你的任务很重要，可以使用下面的代码，以牺牲遮罩和未遮罩区域之间一些不自然的过渡，强制保持图像未遮罩区域不变。
```python
import PIL
import numpy as np
import torch

from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

device = "cuda"
pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    torch_dtype=torch.float16,
)
pipeline = pipeline.to(device)

img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"

init_image = load_image(img_url).resize((512, 512))
mask_image = load_image(mask_url).resize((512, 512))

prompt = "Face of a yellow cat, high resolution, sitting on a park bench"
repaired_image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]
repaired_image.save("repaired_image.png")

# Convert mask to grayscale NumPy array
mask_image_arr = np.array(mask_image.convert("L"))
# Add a channel dimension to the end of the grayscale mask
mask_image_arr = mask_image_arr[:, :, None]
# Binarize the mask: 1s correspond to the pixels which are repainted
mask_image_arr = mask_image_arr.astype(np.float32) / 255.0
mask_image_arr[mask_image_arr < 0.5] = 0
mask_image_arr[mask_image_arr >= 0.5] = 1

# Take the masked pixels from the repainted image and the unmasked pixels from the initial image
unmasked_unchanged_image_arr = (1 - mask_image_arr) * init_image + mask_image_arr * repaired_image
unmasked_unchanged_image = PIL.Image.fromarray(unmasked_unchanged_image_arr.round().astype("uint8"))
unmasked_unchanged_image.save("force_unmasked_unchanged.png")
make_image_grid([init_image, mask_image, repaired_image, unmasked_unchanged_image], rows=2, cols=2)
```

#### 3.6.4 配置pipeline参数
图像特征，如质量和“创造性”，取决于pipeline参数。关于pipeline参数的更多细节，请参见3.5.3节。本节仅给出相关示例代码。
### 1. strength参数
关于strength参数的更多信息，请参见3.5.3节。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, strength=0.6).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```

![image](https://github.com/user-attachments/assets/671cd1d6-881f-4648-b082-a885e91fc67b)


运行上述代码，生成的图像如图3 - 61~图3 - 63所示。
- 图3 - 61 strength为0.6时模型生成的图像
- 图3 - 62 strength为0.8时模型生成的图像
- 图3 - 63 strength为1.0时模型生成的图像
### 2. guidance_scale参数
更多关于guidance_scale参数的信息，请参见3.4.2节和3.5.3节。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, guidance_scale=2.5).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
运行上述代码，生成的图像如图3 - 64~图3 - 66所示。
- 图3 - 64 guidance_scale = 2.5时模型生成的图像
- 图3 - 65 guidance_scale = 7.5时模型生成的图像
- 图3 - 66 guidance_scale = 12.5时模型生成的图像

![image](https://github.com/user-attachments/assets/1167b111-132a-4381-ba27-308542b5f2cd)


### 3. negative_prompt参数
更多关于negative_prompt参数的信息，请参见3.4.2节和3.5.3节。示例代码如下。 

```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
negative_prompt = "bad architecture, unstable, poor details, blurry"
image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
运行上述代码，生成的图像如图3 - 67所示。

![图3-67 negative_prompt="bad architecture, unstable, poor details, blurry"时模型生成的图像](此处无实际图片内容，按原文描述应是对应模型生成图像相关图像)

![image](https://github.com/user-attachments/assets/b2e15427-c860-43cd-ad8e-fd548d3309ef)


### 3.6.5 串联修复pipeline
AutoPipelineForInpainting可以与其他Diffusers pipeline连锁，以编辑它们的输出。这通常有助于提高其他Diffusers pipeline的输出质量，如果你正在使用多个pipeline，将它们连锁在一起以保持输出在潜在空间，并重用相同的pipeline组件，可以更节省内存。

将文本到图像和修复pipeline连锁，可以对生成的图像进行修复，而无须提供基础图像。这让用户方便编辑自己喜爱的文本到图像输出，无须生成全新的图像。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForText2Image, AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

text2image = pipeline("concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k").images[0]
```
从之前的步骤加载遮罩图像的输出可以使用如下代码。
```python
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_text-chain-mask.png")
```
为了在图像中的遮罩区域修复一座瀑布，可以使用如下代码。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder-inpaint", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

prompt = "digital painting of a fantasy waterfall, cloudy"
image = pipeline(prompt=prompt, image=text2image, mask_image=mask_image).images[0]
make_image_grid([text2image, mask_image, image], rows=1, cols=3)
```
运行上述代码，生成的图像如图3 - 68和图3 - 69所示。
- 图3 - 68 文本到图像生成的图像
- 图3 - 69 inpaint生成的图像

![image](https://github.com/user-attachments/assets/e370ce81-0c9e-40a4-86a4-8af8d505a877)


你还可以在其他pipeline（如图像到图像或增强器）之前连锁一个修复pipeline，以提高质量。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting, AutoPipelineForImage2Image
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image_inpainting = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]

# resize image to 1024x1024 for SDXL
image_inpainting = image_inpainting.resize((1024, 1024))
```
现在，将图像传递给另一个带有Stable Diffusion XL细化模型的修复pipeline以提高图像细节和质量。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting

pipeline = AutoPipelineForInpainting.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image = pipeline(prompt=prompt, image=image_inpainting, mask_image=mask_image, output_type="latent").images[0]
```
最后，你可以将这幅图像传递给图像到图像pipeline，以完成润色工作。使用from_pipe方法重用现有的pipeline组件更有效，可以避免再次将所有pipeline组件加载到内存中。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image

pipeline = AutoPipelineForImage2Image.from_pipe(pipeline)
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image = pipeline(prompt=prompt, image=image).images[0]
make_image_grid([init_image, mask_image, image_inpainting, image], rows=2, cols=2)
```
相关的图像如图3 - 70~图3 - 72所示。
- 图3 - 70 初始图像
- 图3 - 71 修复图像
- 图3 - 72 图像到图像生成的图像

![image](https://github.com/user-attachments/assets/c835e871-ebc9-4302-a10e-c3fbcf96cbde)


图像到图像和图像修复实际上是非常相似的任务。图像到图像生成一个类似于现有提供图像的新图像。图像修复可以完成同样的事情，但它只转换由遮罩定义的图像区域，其余图像保持不变。你可以将图像修复视为进行特定更改的更精确工具，不过图像到图像转换具有更广泛的应用范围，用于进行更全面的变更。

### 3.6.6 控制图像生成
获取一个完全符合你期望的图像很具挑战性，因为去噪过程是随机的。虽然你可以通过设置参数（如negative_prompt）来控制生成的某些方面，但还有更好、更有效的方法来控制图像生成。
### 1. 提示加权
关于提示加权的更多信息，请参见3.4.3节。Compel库提供了一个直观的语法来缩放提示加权并生成嵌入。在提示加权指南中可了解如何创建嵌入。

可以将嵌入传递给AutoPipelineForInpainting中的prompt_embeds（如果使用negative_prompt，则为negative_prompt_embeds）参数，它能替代prompt参数。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForInpainting
from diffusers.utils import make_image_grid

pipeline = AutoPipelineForInpainting.from_pretrained(
    "runwayml/stable-diffusion-inpainting", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

image = pipeline(prompt_embeds=prompt_embeds,  # generated from Compel
                 negative_prompt_embeds=negative_prompt_embeds,  # generated from Compel
                 image=init_image,
                 mask_image=mask_image
                 ).images[0]
make_image_grid([init_image, mask_image, image], rows=1, cols=3)
```
### 2. ControlNet
关于ControlNet的更多信息，请参见3.4.3节。这里通过一个示例来介绍。

首先，用一个在修复图像上预训练的ControlNet来条件化一幅图像。示例代码如下。
```python
import torch
import numpy as np
from diffusers import ControlNetModel, StableDiffusionControlNetInpaintPipeline
from diffusers.utils import load_image, make_image_grid

# load ControlNet
controlnet = ControlNetModel.from_pretrained("lllyasviel/control_v11p_sd15_inpaint", torch_dtype=torch.float16, variant="fp16")

# pass ControlNet to the pipeline
pipeline = StableDiffusionControlNetInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-inpainting", controlnet=controlnet, torch_dtype=torch.float16, variant="fp16"
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# load base and mask image
init_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint.png")
mask_image = load_image("https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/inpaint_mask.png")

# prepare control image
def make_inpaint_condition(init_image, mask_image):
    init_image = np.array(init_image.convert("RGB")).astype(np.float32) / 255.0
    mask_image = np.array(mask_image.convert("L")).astype(np.float32) / 255.0

    assert init_image.shape[0:1] == mask_image.shape[0:1], "image and image mask must have the same image size"
    init_image[mask_image > 0.5] = -1.0  # set as masked pixel
    init_image = np.expand_dims(init_image, 0).transpose(0, 3, 1, 2)
    init_image = torch.from_numpy(init_image)
    return init_image

control_image = make_inpaint_condition(init_image, mask_image)
```
然后，基于初始图像、遮罩图像和控制图像生成一幅图像。示例代码如下。
```python
prompt = "concept art digital painting of an elven castle, inspired by lord of the rings, highly detailed, 8k"
image = pipeline(prompt=prompt, image=init_image, mask_image=mask_image, control_image=control_image).images[0]
make_image_grid([init_image, mask_image, PIL.Image.fromarray(np.uint8(control_image[0].numpy())).convert('RGB'), image], rows=2, cols=2)
```
还可以进一步将其与图像到图像的pipeline连锁，以应用新风格。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "nitrosocke/elden-ring-diffusion", torch_dtype=torch.float16
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

prompt = "elden ring style castle"  # include the token "elden ring style" in the prompt
negative_prompt = "bad architecture, deformed, disfigured, poor details"

image_elden_ring = pipeline(prompt, negative_prompt=negative_prompt, image=image).images[0]
make_image_grid([init_image, mask_image, image, image_elden_ring], rows=2, cols=2)
```
运行上述代码，相关的图像如图3 - 73~图3 - 75所示。
- 图3 - 73 初始图像
- 图3 - 74 ControlNet修复的图像
- 图3 - 75 图生图

![image](https://github.com/user-attachments/assets/ea448147-1093-47e6-9f4b-209d49158e7f)


### 3.7 小结
本章对Stable Diffusion进行了全面的探讨，这是一款利用先进人工智能技术，为数字艺术创作带来革命性变化的工具。通过深入探索其概念基础、实际应用，以及智能技术，如文本到图像转换和图像修补等创新功能，让读者对Stable Diffusion的能力及其在各行业中的应用有了全面了解。

另外还展示了Stable Diffusion在艺术和技术融合方面的潜力。通过其在创意艺术领域的杰出代表，不仅提升了当前的数字艺术实践，也为未来的创新开辟了新途径。未来的研究可以进一步探索其与其他技术的持续发展、Stable Diffusion等工具在塑造未来数字内容创作和艺术表达的过程中发挥至关重要的作用。通过不断对其进行研究和应用，我们将能更好地理解和利用这些强大的AI工具，为创意产业带来更广阔的可能性。 
