![图3-25 Kandinsky 2.2生成的图像](此处无实际图片内容，按原文描述应是Kandinsky 2.2生成的图像相关图像)

![image](https://github.com/user-attachments/assets/62fdcaa2-0494-4e24-a51f-3b75e666fe80)


### 4. ControlNet

ControlNet模型是辅助模型或适配器，它们在文本到图像模型（如Stable Diffusion v1.5）的基础上进行了微调。通过将ControlNet模型与文本到图像模型结合使用，可以获得更多显式控制图像生成方式的多样化选项。在使用ControlNet时，需要向模型添加一个额外的条件输入图像。例如，如果你提供一个人体姿势图像（通常表示为由多个关键点连接而成的骨架）作为条件输入，模型将生成一个遵循该姿势的图像。

在这个例子中，我们以一个人体姿势估计图像作为ControlNet的条件。首先，加载预训练于人体姿势估计的ControlNet模型。示例代码如下。
```python
from diffusers import ControlNetModel, AutoPipelineForText2Image
from diffusers.utils import load_image
import torch

controlnet = ControlNetModel.from_pretrained(
    "lllyasviel/control_v1p_sd15_openpose", torch_dtype=torch.float16,
variant="fp16"
).to("cuda")
pose_image = load_image("https://huggingface.co/lllyasviel/control_v1p_sd15_openpose/resolve/main/images/control.png")
```
接下来，将ControlNet传递给AutoPipelineForText2Image，并给出提示和姿势估计图像。示例代码如下。
```python
pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", controlnet=controlnet, torch_dtype=torch.float16, variant="fp16"
).to("cuda")
generator = torch.Generator("cuda").manual_seed(31)
image = pipeline("Astronaut in a jungle, cold color palette, muted colors, detailed, 8k", image=pose_image, generator=generator).images[0]
image
```
运行上述代码，输出如图3 - 26所示。

![图3-26 ControlNet (pose conditioning)生成的图像](此处无实际图片内容，按原文描述应是ControlNet (pose conditioning)生成的图像相关图像)

![image](https://github.com/user-attachments/assets/43c64f6f-acba-4cbd-bf66-d48c9ebb7214)


#### 3.4.2 配置pipeline参数
在pipeline中，可以配置的参数有许多。这些参数影响图像的生成方式。通过修改这些参数，可以更改图像的输出大小，指定负面提示以提高图像质量等。本节将深入探讨如何使用这些参数。
### 1. height和width参数
height（高度）和width（宽度）参数控制生成图像的高度和宽度（以像素为单位）。在默认情况下，Stable Diffusion v1.5模型输出的图像大小为512像素×512像素，但读者可以将其更改为任何8的倍数的大小。例如，可以使用如下代码创建一幅矩形图像。
```python
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16,
variant="fp16"
).to("cuda")
image = pipeline(
    "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
height=768, width=512
).images[0]
image
```
运行上述代码，输出如图3 - 27所示。

![image](https://github.com/user-attachments/assets/e25b1194-5acb-44b6-83bb-accda7608b6f)


![图3-27 输出矩形图像](此处无实际图片内容，按原文描述应是输出矩形图像相关图像)

**小提示**：其他模型可能会根据训练数据集中的图像大小提供不同的默认图像尺寸。例如，Stable Diffusion XL的默认图像大小是1024像素×1024像素，使用较低的高度值和宽度值可能会导致图像质量下降。在使用前，请确保先查阅模型的API参考文档。

### 2. guidance_scale参数

guidance_scale（引导比例）参数影响提示对图像生成的影响程度。较低的guidance_scale值赋予模型“创造力”，生成与提示关系更松散的图像。较高的guidance_scale值则促使模型更紧密地遵循提示，如果这个值过高，你可能会在生成的图像中观察到一些人工痕迹。示例代码如下。

```python
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
image = pipeline(
    "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
guidance_scale=3.5
).images[0]
image
```
图3 - 28~图3 - 30展示了guidance_scale分别为3.5、7.5和10时模型生成的图像。
- 图3 - 28 guidance_scale = 3.5时模型生成的图像
- 图3 - 29 guidance_scale = 7.5时模型生成的图像
- 图3 - 30 guidance_scale = 10时模型生成的图像

![image](https://github.com/user-attachments/assets/6048671e-8350-4f11-8745-16bbb641a4ba)


### 3. negative_prompt参数
就像提示指导图像生成一样，negative_prompt参数可以引导模型避开你不希望生成的内容。这通常用于通过去除诸如“低分辨率”或“细节差”等不良或不佳的图像特征来提高整体图像质量。你还可以使用负面提示来移除或修改图像的内容和风格。示例代码如下。
```python
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
image = pipeline(
    "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
prompt="Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
negative_prompt="ugly, deformed, disfigured, poor details, bad anatomy"
).images[0]
image
```
使用不同negative_prompt参数的模型生成的图像如图3 - 31和图3 - 32所示，我们可以从中清晰地看出差别。

![image](https://github.com/user-attachments/assets/ec2d7ca1-7bf7-40c7-9ff8-826250723db6)


- 图3 - 31 negative_prompt = "ugly, deformed, disfigured, poor details, bad anatomy"时模型生成的图像
- 图3 - 32 negative_prompt = "astronaut"时模型生成的图像
### 4. generator参数
在pipeline中使用torch.Generator对象可以通过设置手动种子来实现可重复性。你可以使用generator（生成器）参数生成图像批次，并根据《使用确定性生成提高图像质量》指南中的详细说明，对基于种子生成的图像进行迭代改进。

你可以按照下面的方式设置种子和generator。使用generator创建的图像应该每次返回相同的结果，而不是随机生成新图像。
```python
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
generator = torch.Generator(device="cuda").manual_seed(30)
image = pipeline(
    "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k",
    generator=generator,
).images[0]
image
```

#### 3.4.3 控制图像生成
除了配置pipeline参数以外，还有几种方法可以更多地控制图像的生成，例如提示加权和使用ControlNet模型。这些方法提供了额外的手段来精确地指导和影响图像的生成过程。
### 1. 提示加权
提示加权（prompt weighting）是一种用于增加或减少提示中概念的重要性的技术，通常用于强调或最小化图像中的某些特征，可以使用Compel库来帮助生成加权的提示嵌入。通过这种方法，你可以更精确地调整图像中的特定细节或特征。

创建嵌入后，可以将它们传递给pipeline中的prompt_embeds参数（如果使用负面提示，还可以使用negative_prompt_embeds参数）。示例代码如下。
```python
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
).to("cuda")
image = pipeline(
    prompt_embeds=prompt_embeds, # generated from Compel
    negative_prompt_embeds=negative_prompt_embeds, # generated from Compel
).images[0]
```
### 2. ControlNet
在3.4.1节中，我们了解到这些模型通过加入额外的条件图像输入，提供了更灵活、准确的图像生成方式。每个ControlNet模型都预先训练在特定类型的条件图像上，以生成类似的新图像。例如，使用在深度图上预训练的ControlNet模型，可以给模型一个深度图作为条件输入，它将生成保留空间信息的图像。这比在提示中指定深度信息更快、更简单，你甚至可以用MultiControlNet结合多个条件输入！

当然，你也可以使用多种类型的条件输入，Diffusers支持Stable Diffusion和Stable Diffusion XL的ControlNet。你可以通过查看更全面的ControlNet使用指南来进一步使用这些模型。

#### 3.4.4 优化操作
Stable Diffusion的体积庞大，去噪图像的迭代本质在计算层面上需要大量的算力。但这不意味着你需要算力强大的GPU或很多GPU来运行它们，在使用消费者和免费层资源运行Stable Diffusion时，有许多优化技巧。例如，可以以半精度加载模型权重，从而节省GPU内存并提高速度，或将整个模型下载到GPU以节省更多内存。

PyTorch 2.0还支持一种更节省内存的注意力机制，称为缩放点积注意力（scaled dot - product attention），如果你使用PyTorch 2.0，它会自动启用。同时，还可以结合torch.compile进一步加快代码运行速度。示例代码如下。
```python
from diffusers import AutoPipelineForText2Image
import torch

pipeline = AutoPipelineForText2Image.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16").to("cuda")
pipeline.unet = torch.compile(pipeline.unet, mode="reduce-overhead", fullgraph=True)
```

### 3.5 图生图
图生图类似于文本到图像，但除一个提示以外，你还可以传递一个初始图像作为扩散过程的起点。初始图像被编码到潜在空间中，并向其添加噪声。然后，潜在扩散模型接受一个提示和噪声潜在图像，预测添加的噪声，并从初始潜在图像中移除预测的噪声，以获得新的潜在图像。最后，一个解码器将新的潜在图像解码回图像。

#### 3.5.1 快速入门
我们使用Hugging Face的Diffuser来实现此过程，具体步骤如下。
**第1步**，将checkpoint加载到AutoPipelineForImage2Image类中。这个pipeline会基于checkpoint自动加载正确的pipeline类。示例代码如下。 



```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import load_image, make_image_grid

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# 如果未安装xFormers或安装PyTorch 2.0及更高版本，则移除下面这行代码
pipeline.enable_xformers_memory_efficient_attention()
```

**小提示**：本书使用了enable_model_cpu_offload和enable_xformers_memory_efficient_attention函数来节省内存并提高推理速度。如果你正在使用PyTorch 2.0，那么无须在pipeline上调用enable_xformers_memory_efficient_attention函数，因为它已经支持PyTorch 2.0的原生缩放点积注意力机制。

**第2步**，加载一幅图像（见图3 - 33）并传递给pipeline。示例代码如下。
```python
init_image = load_image("https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg")
```

**第3步**，传递一个提示和图像给pipeline以生成一幅图像（见图3 - 34）。示例代码如下。
```python
prompt = "ghibli style, a fantasy landscape with castles"
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
![图3-33 初始图像](此处无实际图片内容，按原文描述应是初始图像相关图像)
![图3-34 生成的图像](此处无实际图片内容，按原文描述应是生成的图像相关图像)

![image](https://github.com/user-attachments/assets/73552098-1da5-4e37-9291-2d19bd478a38)


#### 3.5.2 流行的模型
最受欢迎的图像到图像模型是Stable Diffusion v1.5、Stable Diffusion XL和Kandinsky 2.2。由于这些模型的架构和训练过程不同，Stable Diffusion和Kandinsky模型的结果也不相同。一般而言，你可以期待Stable Diffusion XL产生比Stable Diffusion v1.5更高质量的图像。
### 1. Stable Diffusion v1.5
关于Stable Diffusion v1.5的更多信息，请参见3.4.1节。在使用Stable Diffusion v1.5时，首先需要准备一幅初始图像并传递给pipeline，然后传递一个提示和图像给pipeline以生成新图像。示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
# prompt = "太空人在丛林中，冷色调，柔和色彩，详细，8k"
# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
运行上述代码，初始图像与生成的图像的对比如图3 - 35所示。

![image](https://github.com/user-attachments/assets/7a83ced0-6294-4964-97c7-385b435b8d8a)


![图3-35 初始图像与生成的图像的对比](此处无实际图片内容，按原文描述应是初始图像与生成的图像的对比相关图像)
### 2. Stable Diffusion XL

关于Stable Diffusion XL的更多信息，请参见3.4.1节。使用这个模型的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# 如果未安装xFormers或安装PyTorch 2.0及更高版本，则移除下面这行代码
pipeline.enable_xformers_memory_efficient_attention()

# 准备图像
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-sdxl-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
# prompt = "太空人在丛林中，冷色调，柔和色彩，详细，8k"

# 传递提示和图像给pipeline
image = pipeline(prompt, image=init_image, strength=0.5).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
运行上述代码，初始图像与生成的图像的对比如图3 - 36所示。

![图3-36 初始图像与生成的图像的对比](此处无实际图片内容，按原文描述应是初始图像与生成的图像的对比相关图像)

![image](https://github.com/user-attachments/assets/635097d9-6b4b-4e39-b470-2dce74fb8e2b)


### 3. Kandinsky 2.2
关于Kandinsky 2.2的更多信息，请参见3.4.1节。使用这个模型的示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "kandinsky-community/kandinsky-2-2-decoder", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# 如果未安装xFormers或安装PyTorch 2.0及更高版本，则移除下面这行代码
pipeline.enable_xformers_memory_efficient_attention()

# 准备图像
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"
#prompt = "太空人在丛林中，冷色调，柔和色彩，详细，8k"

# 传递提示和图像给pipeline
image = pipeline(prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
运行上述代码，初始图像与生成的图像的对比如图3 - 37所示。

![图3-37 初始图像与生成的图像的对比](此处无实际图片内容，按原文描述应是初始图像与生成的图像的对比相关图像)

![image](https://github.com/user-attachments/assets/a7263a08-7f65-4160-b65d-b7baea964768)


#### 3.5.3 配置pipeline参数
在pipeline中包含多个重要参数，这些参数会影响图像生成过程和图像质量。下面将介绍这些参数及其使用方式。
### 1. strength参数
strength（强度）参数是需要考虑的最重要的参数之一，它对生成的图像有巨大影响。它决定了生成的图像与初始图像的相似程度。
- 更高的strength值给予模型更多“创造力”来生成与初始图像不同的图像。strength值为1.0，意味着基本忽略初始图像。 
- 更低的strength值意味着生成的图像更接近初始图像。

strength和num_inference_steps参数是相关的，因为strength决定了要添加的噪声步骤数量。如果num_inference_steps为50且strength为0.8，则意味着向初始图像添加40步（50×0.8）的噪声，然后进行40步去噪以获取新生成的图像，示例代码如下。
```python
import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import make_image_grid, load_image

pipeline = AutoPipelineForImage2Image.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16, variant="fp16", use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

# prepare image
url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/img2img-init.png"
init_image = load_image(url)

prompt = "Astronaut in a jungle, cold color palette, muted colors, detailed, 8k"

# pass prompt and image to pipeline
image = pipeline(prompt, image=init_image, strength=0.8).images[0]
make_image_grid([init_image, image], rows=1, cols=2)
```
如图3 - 38~图3 - 40所示，随着strength值增加，生成的图像与初始图像的差别越来越大，同时越来越接近提示的内容。

- 图3 - 38 strength为0.4时初始图像与生成的图像的对比

![image](https://github.com/user-attachments/assets/ce91d5ce-b79a-411f-860b-43ce3f11efdd)

  
- 图3 - 39 strength为0.6时初始图像与生成的图像的对比

![image](https://github.com/user-attachments/assets/9ff54653-53d9-4919-aabd-8102d29582bd)



- 图3 - 40 strength为0.8时初始图像与生成的图像的对比


![image](https://github.com/user-attachments/assets/c551a331-0229-4601-bc39-3343aebdf066)

### 2. guidance_scale参数
更多关于guidance_scale参数的信息，请参见3.4.2节。你可以将guidance_scale与strength结合使用，以获得对模型表达能力的更精确控制。例如，结合使用高strength和高guidance_scale以获得最大创造力，或者使用低strength和低guidance_scale的组合来生成一个类似于初始图像的图像。 
