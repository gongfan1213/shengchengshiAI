### 5. 检索和生成：生成
现在让我们将所有内容整合到一个链条中，该链条将接收一个问题，检索相关文档，构造提示，然后将其传递给一个模型，并解析输出。

我们将使用gpt - 3.5 - turbo OpenAI聊天模型，但可以用任何LangChain的大语言模型或ChatModel来替代。示例代码如下。
### Python
```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
```
我们将使用一个存储于LangChain提示中心的用于RAG的提示作为示例。示例代码如下。
### Python
```python
from langchain import hub

prompt = hub.pull("rlm/rag-prompt")

example_messages = prompt.invoke(
    {"context": "filler context", "question": "filler question"}
).to_messages()
example_messages

# [HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: filler question \nContext: filler context \nAnswer:")]
print(example_messages[0].content)

# You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.
# Question: filler question
# Context: filler context
# Answer:
```
通过使用LCEL可运行协议定义该链，我们能够：
- 以透明的方式将组件和函数连接在一起； 
- 在LangSmith中自动跟踪我们构建的链； 
- 使用成熟的接口获得流式、异步和批处理调用。 

示例代码如下。
### Python
```python
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

for chunk in rag_chain.stream("What is Task Decomposition?"):
    print(chunk, end="", flush=True)

# Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for easier interpretation and execution by autonomous agents or models. Task decomposition can be done through various methods, such as using prompting techniques, task-specific instructions, or human inputs.
```
在上述代码中，我们可以从LangChain提示中心加载提示，例如RAG提示。但是这个提示也可以很容易地进行自定义。示例代码如下。
### Python
```python
from langchain_core.prompts import PromptTemplate

template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Use three sentences maximum and keep the answer as concise as possible.
Always say "thanks for asking!" at the end of the answer.

{context}

Question: {question}
Helpful Answer:"""
custom_rag_prompt = PromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | custom_rag_prompt
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What is Task Decomposition?")

# 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'
```
### 4.6 Agent基础应用
当我们明确知道执行特定任务所需的工具使用顺序时，链式操作显得尤为高效。然而，在某些应用场景中，我们使用的工具顺序和次数可能会随着用户输入的变化而变化。针对这类情况，Agent作为一种强大的工具，能够自主决定何时以及如何使用这些工具，从而为我们提供更大的灵活性和适应性。

LangChain配备了众多内置Agent，这些Agent针对不同用例进行了专门的优化。以OpenAI工具Agent为例，它充分利用了最新的OpenAI工具调用API（需注意，此功能仅适用于新型OpenAI模型）。与传统的函数调用方式不同，该API允许模型一次性返回多个函数调用结果，从而显著提升了效率和灵活性。Agent的流程如图4 - 5所示。

![image](https://github.com/user-attachments/assets/8c51eeb3-61ba-4414-923b-1799eb96ff48)


### 图4 - 5 Agent的流程
用户的问题 -> 大语言模型 -> 解析器 -> 工具 -> 决策或预测 -> 用户的输出（循环直至得到最终的回答）
#### 4.6.1 搭建Agent
首先，安装所需的包。示例代码如下。
### Shell
```bash
pip install --upgrade --quiet langchain langchain-openai
```
然后，设置环境变量。示例代码如下。
### Python
```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# 如果想要使用LangSmith，可以取消以下注释
# os.environ["LANGCHAIN_TRACING_V2"] = "true"
# os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```
#### 4.6.2 创建工具
下面创建一些要调用的工具。在这个示例中，我们将从函数创建自定义工具。示例代码如下。
### Python
```python
from langchain_core.tools import tool

@tool
def multiply(first_int: int, second_int: int) -> int:
    """Multiply two integers together."""
    return first_int * second_int

@tool
def add(first_int: int, second_int: int) -> int:
    """Add two integers."""
    return first_int + second_int

@tool
def exponentiate(base: int, exponent: int) -> int:
    """Exponentiate the base to the exponent power."""
    return base**exponent

tools = [multiply, add, exponentiate]
```
#### 4.6.3 创建提示
创建提示的示例代码如下。
### Python
```python
from langchain import hub
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_openai import ChatOpenAI

prompt = hub.pull("hwchase17/openai-tools-agent")
prompt.pretty_print()
```
运行上述代码，输出如下。
### Python
```
===================== System Message =====================
You are a helpful assistant
===================== Messages Placeholder =====================
{chat_history}
===================== Human Message =====================
{input}
===================== Messages Placeholder =====================
{agent_scratchpad}
```
#### 4.6.4 构建Agent
构建Agent的示例代码如下。
### Python
```python
model = ChatOpenAI(model="gpt-3.5-turbo-1106", temperature=0)
agent = create_openai_tools_agent(model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
```
#### 4.6.5 调用Agent
调用Agent的示例代码如下。 （此处未给出具体调用代码，原文可能存在缺失） 

### Python
```python
agent_executor.invoke(
    {
        "input": "Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result."
    }
)
```
运行上述代码，输出如下。
### Python
```
> Entering new AgentExecutor chain...
Invoking: 'exponentiate' with '{'base': 3, 'exponent': 5}'
243
Invoking: 'add' with '{'first_int': 12,'second_int': 3}'
15
Invoking:'multiply' with '{'first_int': 243,'second_int': 15}'
3645
Invoking: 'exponentiate' with '{'base': 3645, 'exponent': 2}'
13286025The result of raising 3 to the fifth power and multiplying that by the sum of twelve and three, then squaring the whole result is 13,286,025.
> Finished chain.
{'input': 'Take 3 to the fifth power and multiply that by the sum of twelve and three, then square the whole result', 'output': 'The result of raising 3 to the fifth power and multiplying that by the sum of twelve and three, then squaring the whole result is 13,286,025.'}
```
### 4.7 高级RAG应用程序
前面介绍了一个基础的RAG应用程序，本节将继续深入这个应用程序，并通过在其上添加不同类型的新功能来介绍更多LangChain组件的使用方式，包括返回信息源、添加聊天记录等。
#### 4.7.1 高级应用示例——添加聊天记录
在许多QA应用程序中，我们希望允许用户进行来回对话，这意味着应用程序需要一些“记忆”来记录过去的问题和答案，并且需要一些逻辑来将它们融入当前的思考中。在本书中，我们重点介绍如何添加逻辑将历史消息纳入考虑，而不是聊天历史管理。

接下来，需要更新4.6节开发的应用程序的如下两个方面。
- **提示**：更新提示以支持将历史消息作为输入。 
- **问题的情境化**：添加一个子链，接收最新的用户问题并将其在聊天历史的背景下重新构造。这是必要的，因为最新的问题可能参考了过去消息的某些上下文。例如，如果用户提出了一个后续问题，比如“你能详细解释第二点吗？”，在没有上下文的情况下程序将无法理解。因此，我们无法有效地使用这样的问题进行检索。 
#### 4.7.2 构建环境
### 1. 依赖模块
我们将在本书中使用OpenAI聊天模型和嵌入以及Chroma向量存储，但是这里展示的所有内容适用于任何ChatModel或大语言模型、Embeddings和VectorStore或Retriever。

首先，导入软件包。示例代码如下。
### Python
```bash
pip install --upgrade --quiet langchain langchain-community langchainhub langchain-openai chromadb bs4
```
然后，设置环境变量OPENAI_API_KEY。可以直接设置，也可以从.env文件中加载。示例代码如下。
### Python
```python
import getpass
import os

os.environ["OPENAI_API_KEY"] = getpass.getpass()

# import dotenv
# dotenv.load_dotenv()
```
### 2. LangSmith
在使用LangChain构建的许多应用程序中，可能会涉及多个步骤和多次大型语言模型调用的情况。随着这些应用程序变得越来越复杂，能够检查链或Agent程序内部发生的情况变得至关重要。LangSmith是实现这一目标的理想工具。

需要注意的是，虽然LangSmith并非必需，但它对于跟踪和分析链或Agent程序内部发生的情况非常有帮助。如果你打算使用LangSmith，请在注册后确保正确设置环境变量，以便开始记录跟踪信息。这将有助于你更好地理解程序的运行情况，从而优化和改进应用程序的性能。示例代码如下。
### Python
```python
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = getpass.getpass()
```
#### 4.7.3 没有聊天历史的链条
本节将针对基于Lilian Weng的“LLM Powered Autonomous Agents”博文中构建的QA应用程序进行介绍。

首先，导入需要的包。示例代码如下。
### Python
```python
import bs4
from langchain import hub
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
```
其次，构建不包含聊天历史的链条。示例代码如下。
### Python
```python
# Load, chunk and index the contents of the blog
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/,"),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    )
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())

# Retrieve and generate using the relevant snippets of the blog
retriever = vectorstore.as_retriever()
prompt = hub.pull("rlm/rag-prompt")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

rag_chain.invoke("What is Task Decomposition?")

# 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.'
```
#### 4.7.4 情境化问题
首先，需要定义一个子链，该子链接历史消息和最新的用户问题，并在遇到新问题时参考历史信息对问题进行重新构造。

我们将使用一个包含MessagesPlaceholder变量的提示，名称为“chat_history”。这允许我们通过使用“chat_history”输入键将消息列表传递到提示中，这些消息将被插入到系统消息之后和包含最新问题的人类消息之前。示例代码如下。
### Python
```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

contextualize_q_system_prompt = """Given a chat history and the latest user question \
which might reference context in the chat history, formulate a standalone question \
which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is."""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)
contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()
```
通过使用这个链，我们可以提出引用过去消息的后续问题，并将它们重新构造成独立的问题。示例代码如下。
### Python
```python
from langchain_core.messages import AIMessage, HumanMessage

contextualize_q_chain.invoke(
    {
        "chat_history": [
            HumanMessage(content="What does LLM stand for?"),
            AIMessage(content="Large language model"),
        ],
        "question": "What is meant by large",
    }
)

# 'What is the definition of "large" in the context of a language model?'
```
#### 4.7.5 有聊天历史的链条
现在我们可以构建一个完整的问答链，其中包含了一些路由功能。这些功能确保了只有在聊天历史不为空时，才会执行“condense question chain”。在这个过程中，我们利用了一个关键事实：如果LCEL链中的一个函数返回另一个链，那么该链本身将被调用。这种方式使得我们能够灵活地组织和调用链式操作，从而实现更高效的问答处理。示例代码如下。
### Python
```python
qa_system_prompt = """You are an assistant for question-answering tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Use three sentences maximum and keep the answer concise.\
{context}"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)

def contextualized_question(input: dict):
    if input.get("chat_history"):
        return contextualize_q_chain
    else:
        return input["question"]

rag_chain = (
    RunnablePassthrough.assign(
        context=contextualized_question | retriever | format_docs
    )
    | qa_prompt
    | llm
)

chat_history = []
question = "What is Task Decomposition?"
ai_msg = rag_chain.invoke({"question": question, "chat_history": chat_history})
chat_history.extend([HumanMessage(content=question), ai_msg])

second_question = "What are common ways of doing it?"
rag_chain.invoke({"question": second_question, "chat_history": chat_history})

# AIMessage(content='Common ways of task decomposition include:\n\n1. Chain of Thought (CoT): CoT is a prompting technique that instructs the model to "think step by step" and decompose complex tasks into smaller and simpler steps. This approach utilizes the model\'s thinking process at test-time and sheds light on the model\'s thought process.\n\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like "Steps for XYZ" or "What are the subgoals for achieving XYZ?". This method guides the model to break down the task into manageable steps.\n\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction "Write a story outline" can be given to help the model break down the task into smaller components.\n\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide insights, expertise, and domain knowledge to help break down complex tasks into smaller subtasks.\n\nThese approaches aim to simplify complex tasks and enable more effective problem-solving and planning.')
```
在此，我们已经展示了如何将应用程序逻辑融入以包含历史输出。然而，目前我们仍在手动更新聊天历史，并将其插入到每个输入中。在实际的问答应用程序中，我们需要一种方法来持久化聊天历史，并自动将其插入和更新到每个输入中。这种方法将大大提高问答系统的自动化程度和用户体验。

为此，我们可以使用如下方式。
- **BaseChatMessageHistory**：存储聊天历史记录。 
- **RunnableWithMessageHistory**：对LCEL链和BaseChatMessageHistory进行封装，将聊天历史记录注入输入并在每次调用后更新它的包装器。 
### 4.8 小结
本章全面介绍了LangChain及其组件、LCEL环境以及如何通过LangChain实现AI Agent和RAG应用程序开发。从基础组件的描述、AI Agent的集成应用，到具体的应用案例分析，这些文件涵盖了LangChain生态系统的多个方面。它们不仅提供了关于LangChain如何增强NLP和AI应用程序开发的深入理解，还展示了通过实际案例如何将理论应用到实践中，为开发者提供了一套完整的工具和方法论来探索和实现基于LangChain的项目。

通过深入学习和使用LangChain及其相关组件，未来展望呈现了一片广阔的天地。随着技术的不断进步，我们可以预见到LangChain在NLP和AI领域将扮演更加重要的角色，尤其是在提升AI Agent的能力、优化NLP应用程序以及加速人机交互进程方面。未来，LangChain可能会引入更多创新的算法和模型，进一步简化开发流程，使非专业人士也能轻松构建复杂的NLP应用程序。此外，随着社区的不断壮大，我们还将看到更多基于LangChain的开源项目和商业应用诞生，这将极大地促进知识共享和技术创新。在这样一个快速发展的生态系统中，LangChain及其组件无疑将成为推动未来语言技术发展的重要力量。 

